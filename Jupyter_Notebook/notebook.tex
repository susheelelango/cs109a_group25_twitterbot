
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{cs109a\_group25\_tweet\_bots}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} RUN THIS CELL FOR FORMAT}
        \PY{k+kn}{import} \PY{n+nn}{requests}
        \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{core}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{HTML}
        \PY{n}{styles} \PY{o}{=} \PY{n}{requests}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{https://raw.githubusercontent.com/Harvard\PYZhy{}IACS/2018\PYZhy{}CS109A/master/content/styles/cs109.css}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{text}
        \PY{n}{HTML}\PY{p}{(}\PY{n}{styles}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}1}]:} <IPython.core.display.HTML object>
\end{Verbatim}
            
    \section{ CS109A Introduction to Data
Science}\label{cs109a-introduction-to-data-science}

\textbf{Harvard University} \textbf{Fall 2018} \textbf{Instructors}:
Pavlos Protopapas, Kevin Rader

\section{ Final Project: Group 25 - Twitter Bot
Detection}\label{final-project-group-25---twitter-bot-detection}

\textbf{Project Team}: Yankai Su, Susheel Chandar Elango, Mohan Indugula
\textbf{Project Guide}: Brandon Lee

    \subsection{Motivation}\label{motivation}

In today's world where there is thousands of tweets produced every
second, the impact of tweets influencing about various widespread events
ranging from day to day activities, stories, reviews or ratings on a
product/company/individual, perceptions on a public body like a
political party/movement, etc. is very high. The role and usage of
automated accounts known as the social-media bots plays a widespread
role in creating an influence or spreading a message/perception through
content generated with no direct human involvement. Hence, the
importance to identify such \textbf{social-media bots} and tweets
generated by them is important to avoid being influenced by agendas or
perceptions created by such automated accounts.

\subsection{Problem Statement}\label{problem-statement}

This project aims at detecting the Twitter bots i.e. these automated
twitter accounts by using the tweets data generated by these accounts.
The project will use the live twitter data made available through the
Twitter developer API to analyze tweeting patterns of an user and
classify them as a human user or a Twitter bot. The project will take a
\textbf{supervised approach} to perform classification by utilizing a
set of pre-labelled data that already has identified users who are human
users and Twitter bots. This pre-labelled data along with the features
that is extracted from the raw tweets of these users will be used to
train \textbf{classification models} to classify twitter bot users. The
features extracted will also apply \textbf{Natural Language Processing}
techniques to perform \textbf{sentiment analysis and Topic based
Modeling} on the tweet data

\subsection{Implementation Structure of the
Notebook}\label{implementation-structure-of-the-notebook}

The implementation in the notebook has been organized into different
sections as per the Data Science lifecycle process that was followed
during the execution of this project\\
1) Interfacing with Twitter Developer API\\
2) Data Preparation - Includes Raw Data Collection, Data Preprocessing,
Data Cleansing and Standardization, Feature Extraction\\
3) Exploratory Data Analysis\\
4) Model Training and Validation - Includes training and validation of
all the classification models, stacking metalearner and the Topic based
classification model training\\
5) Train and Validation Results and Statistics\\
6) Test Data Preparation and Models Validation using test data set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}154}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
          \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Markdown}\PY{p}{,} \PY{n}{display}
          
          \PY{k+kn}{import} \PY{n+nn}{sys}
          \PY{k+kn}{import} \PY{n+nn}{math}
          \PY{k+kn}{import} \PY{n+nn}{os}
          \PY{k+kn}{import} \PY{n+nn}{random}
          \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{112358}\PY{p}{)}
          \PY{k+kn}{import} \PY{n+nn}{base64}
          \PY{k+kn}{import} \PY{n+nn}{string}
          \PY{k+kn}{import} \PY{n+nn}{re}
          \PY{k+kn}{from} \PY{n+nn}{collections} \PY{k}{import} \PY{n}{Counter}
          
          \PY{k+kn}{import} \PY{n+nn}{csv}
          \PY{k+kn}{import} \PY{n+nn}{jsonpickle}
          \PY{k+kn}{import} \PY{n+nn}{json}
          
          \PY{k+kn}{from} \PY{n+nn}{time} \PY{k}{import} \PY{n}{sleep}
          \PY{k+kn}{from} \PY{n+nn}{datetime} \PY{k}{import} \PY{n}{datetime}
          
          \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
          \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
          \PY{k+kn}{import} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{random} \PY{k}{as} \PY{n+nn}{nd}
          \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k}{import} \PY{n}{expon}
          
          \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
          \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
          
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{base} \PY{k}{import} \PY{n}{TransformerMixin}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{pipeline} \PY{k}{import} \PY{n}{Pipeline}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{MinMaxScaler}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{StandardScaler}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{accuracy\PYZus{}score}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{confusion\PYZus{}matrix}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{RandomizedSearchCV}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegression}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegressionCV}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsClassifier}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{PCA}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{discriminant\PYZus{}analysis} \PY{k}{import} \PY{n}{LinearDiscriminantAnalysis}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{discriminant\PYZus{}analysis} \PY{k}{import} \PY{n}{QuadraticDiscriminantAnalysis}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{SVC}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{LinearSVC}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{import} \PY{n}{DecisionTreeClassifier}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestClassifier}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{AdaBoostClassifier}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{ExtraTreesClassifier}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{GradientBoostingClassifier}
          
          \PY{k+kn}{import} \PY{n+nn}{keras} 
          \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k}{import} \PY{n}{models}
          \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
          \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Flatten}\PY{p}{,} \PY{n}{Dropout}
          \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{constraints} \PY{k}{import} \PY{n}{MaxNorm}
          \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k}{import} \PY{n}{regularizers}
          
          \PY{k+kn}{import} \PY{n+nn}{tweepy} \PY{c+c1}{\PYZsh{} Library to access Twitter Developer API}
          
          \PY{c+c1}{\PYZsh{} Natural Language Processing related Libraries used for Sentiment Analysis and Topic Modeling}
          \PY{k+kn}{from} \PY{n+nn}{textblob} \PY{k}{import} \PY{n}{TextBlob} \PY{c+c1}{\PYZsh{}For Sentiment Analysis}
          
          \PY{k+kn}{import} \PY{n+nn}{nltk} \PY{c+c1}{\PYZsh{} Natural Language Toolkit for Topic Modeling}
          \PY{n}{nltk}\PY{o}{.}\PY{n}{download}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{stopwords}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{k+kn}{import} \PY{n+nn}{spacy} \PY{c+c1}{\PYZsh{} NLP Library}
          \PY{n}{nlp} \PY{o}{=} \PY{n}{spacy}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{en\PYZus{}core\PYZus{}web\PYZus{}sm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{corpus} \PY{k}{import} \PY{n}{stopwords}
          
          \PY{k+kn}{from} \PY{n+nn}{spacy}\PY{n+nn}{.}\PY{n+nn}{lang}\PY{n+nn}{.}\PY{n+nn}{en} \PY{k}{import} \PY{n}{English}
          \PY{n}{parser} \PY{o}{=} \PY{n}{English}\PY{p}{(}\PY{p}{)}
          
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}extraction}\PY{n+nn}{.}\PY{n+nn}{text} \PY{k}{import} \PY{n}{CountVectorizer}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}extraction}\PY{n+nn}{.}\PY{n+nn}{stop\PYZus{}words} \PY{k}{import} \PY{n}{ENGLISH\PYZus{}STOP\PYZus{}WORDS}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[nltk\_data] Downloading package stopwords to
[nltk\_data]     C:\textbackslash{}Users\textbackslash{}tv775c\textbackslash{}AppData\textbackslash{}Roaming\textbackslash{}nltk\_data{\ldots}
[nltk\_data]   Package stopwords is already up-to-date!

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{k}{def} \PY{n+nf}{printmd}\PY{p}{(}\PY{n}{string}\PY{p}{)}\PY{p}{:}
             \PY{n}{display}\PY{p}{(}\PY{n}{Markdown}\PY{p}{(}\PY{n}{string}\PY{p}{)}\PY{p}{)}
             
         \PY{c+c1}{\PYZsh{}Seaborn settings}
         \PY{n}{sns}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{style}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{darkgrid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{flatui} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}34495e}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}1abc9c}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}e74c3c}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}2ecc71}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}9b59b6}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}3498db}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}95a5a6}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}palette}\PY{p}{(}\PY{n}{flatui}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}Pandas Settings}
         \PY{n}{pd}\PY{o}{.}\PY{n}{set\PYZus{}option}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{display.width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{1500}\PY{p}{)}
         \PY{n}{pd}\PY{o}{.}\PY{n}{set\PYZus{}option}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{display.max\PYZus{}columns}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Global Constants}
         \PY{n}{TWEET\PYZus{}EXTRACT\PYZus{}LOCATION\PYZus{}TRAIN} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tweet\PYZus{}extract\PYZus{}data/}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Location of extracted Tweet files for Train and Validation}
         \PY{n}{TWEET\PYZus{}EXTRACT\PYZus{}LOCATION\PYZus{}TEST} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tweet\PYZus{}test\PYZus{}extract\PYZus{}data/}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Location of extracted Tweet files for Test Data set}
         \PY{n}{DATA\PYZus{}FILE\PYZus{}SUFFIX} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.dat}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Extension used for the JSON data files}
         \PY{n}{SUCCESS\PYZus{}USER\PYZus{}STATS} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{success\PYZus{}user\PYZus{}stats.csv}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Data file containing list of users for whom valid data was collected}
         \PY{n}{RESTRICTED\PYZus{}USER\PYZus{}STATS} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{restricted\PYZus{}user\PYZus{}stats.csv}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Data file containing list of restricted users}
         \PY{n}{CONSOLIDATED\PYZus{}FEATURES} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{consolidated\PYZus{}features.csv}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Data file containing list of consolidated features for all users}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{k+kn}{import} \PY{n+nn}{warnings}
         \PY{n}{warnings}\PY{o}{.}\PY{n}{filterwarnings}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ignore}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{category}\PY{o}{=}\PY{n+ne}{UserWarning}\PY{p}{,} \PY{n}{module}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sklearn}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \section{ Instantiate Connection with Twitter Developer
API}\label{instantiate-connection-with-twitter-developer-api}

\textbf{Use the API Key and Secret code provided by twitter for the app
subscription and initiate a connection using tweepy}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{c+c1}{\PYZsh{} Replace the API\PYZus{}KEY and API\PYZus{}SECRET with your application\PYZsq{}s key and secret.}
         \PY{n}{API\PYZus{}KEY} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZlt{}USE\PYZus{}SUBSCRIPTION\PYZus{}APP\PYZus{}API\PYZus{}KEY\PYZgt{}}\PY{l+s+s2}{\PYZdq{}}
         \PY{n}{API\PYZus{}SECRET} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZlt{}USE\PYZus{}SUBSCRIPTION\PYZus{}APP\PYZus{}API\PYZus{}SECRET\PYZgt{}}\PY{l+s+s2}{\PYZdq{}}
         
         \PY{c+c1}{\PYZsh{} Authenticate user API Key and Secret code}
         \PY{n}{auth} \PY{o}{=} \PY{n}{tweepy}\PY{o}{.}\PY{n}{AppAuthHandler}\PY{p}{(}\PY{n}{API\PYZus{}KEY}\PY{p}{,} \PY{n}{API\PYZus{}SECRET}\PY{p}{)}
         \PY{n}{api} \PY{o}{=} \PY{n}{tweepy}\PY{o}{.}\PY{n}{API}\PY{p}{(}\PY{n}{auth}\PY{p}{,} \PY{n}{wait\PYZus{}on\PYZus{}rate\PYZus{}limit}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{wait\PYZus{}on\PYZus{}rate\PYZus{}limit\PYZus{}notify}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
          
         \PY{k}{if} \PY{p}{(}\PY{o+ow}{not} \PY{n}{api}\PY{p}{)}\PY{p}{:}
             \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Can}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t Authenticate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{sys}\PY{o}{.}\PY{n}{exit}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{c+c1}{\PYZsh{} Method Implementation to Fetch tweets posted by a specified user}
         \PY{c+c1}{\PYZsh{} NOTE: Twitter only allows access to a users most recent 3200 tweets with this method}
         \PY{k}{def} \PY{n+nf}{get\PYZus{}all\PYZus{}tweets\PYZus{}json}\PY{p}{(}\PY{n}{user\PYZus{}id}\PY{p}{,} \PY{n}{no\PYZus{}of\PYZus{}tweets}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{,} \PY{n}{tweet\PYZus{}max\PYZus{}id} \PY{o}{=} \PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Fetch the most recent tweets as per no\PYZus{}of\PYZus{}tweets mentioned for the given user\PYZus{}Id from twitter.}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{       Args:}
         \PY{l+s+sd}{           user\PYZus{}id: Twitter User ID of an user as recognized by Twitter developer API.}
         \PY{l+s+sd}{           no\PYZus{}of\PYZus{}tweets: No of recent tweets to fetch for the user. Default set to 200}
         \PY{l+s+sd}{           tweet\PYZus{}max\PYZus{}id: The Max ID of the twitter status that should be fetched}
         \PY{l+s+sd}{                  }
         \PY{l+s+sd}{       Returns:}
         \PY{l+s+sd}{           new\PYZus{}tweets: Array of tweets containing individual tweets in JSON format}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}    
             
             \PY{c+c1}{\PYZsh{} Array initiated to hold the JSON formatted tweets extracted for the user}
             \PY{n}{new\PYZus{}tweets} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{try}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Make initial request for most recent tweets (200 is the maximum allowed count) raised by user}
                 \PY{k}{if} \PY{n}{tweet\PYZus{}max\PYZus{}id} \PY{o}{==} \PY{k+kc}{None}\PY{p}{:}
                     \PY{n}{new\PYZus{}tweets} \PY{o}{=} \PY{n}{api}\PY{o}{.}\PY{n}{user\PYZus{}timeline}\PY{p}{(}\PY{n}{user\PYZus{}id} \PY{o}{=} \PY{n}{user\PYZus{}id} \PY{p}{,}\PY{n}{count} \PY{o}{=} \PY{n}{no\PYZus{}of\PYZus{}tweets}\PY{p}{)}
                 \PY{k}{else}\PY{p}{:}
                     \PY{n}{new\PYZus{}tweets} \PY{o}{=} \PY{n}{api}\PY{o}{.}\PY{n}{user\PYZus{}timeline}\PY{p}{(}\PY{n}{user\PYZus{}id} \PY{o}{=} \PY{n}{user\PYZus{}id} \PY{p}{,}\PY{n}{count} \PY{o}{=} \PY{n}{no\PYZus{}of\PYZus{}tweets}\PY{p}{,} \PY{n}{max\PYZus{}id}\PY{o}{=}\PY{n}{tweet\PYZus{}max\PYZus{}id}\PY{p}{)}
             \PY{k}{except} \PY{n}{tweepy}\PY{o}{.}\PY{n}{TweepError} \PY{k}{as} \PY{n}{e}\PY{p}{:}
                 \PY{k}{raise}
             \PY{k}{except} \PY{n}{tweepy}\PY{o}{.}\PY{n}{RateLimitError} \PY{k}{as} \PY{n}{rate\PYZus{}err}\PY{p}{:}
                 \PY{k}{raise}
                 
             \PY{k}{return} \PY{n}{new\PYZus{}tweets}
\end{Verbatim}


    \section{ Data Preparation}\label{data-preparation}

    Raw Data Collection

\textbf{Read Pre-labelled twitter user set obtained from botometer
(varo-2017)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} \PY{c+c1}{\PYZsh{} Read the Pre\PYZhy{}classified data set containing twitter user labeled as either Bots (1) or Human Users (0)}
         \PY{n}{labelled\PYZus{}df}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{varol\PYZhy{}2017.dat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{delimiter}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,}\PY{n}{header}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Filter the Data frame and limit to the User ID and Bot Flag columns}
         \PY{n}{labelled\PYZus{}df}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} Determine no of users in the labelled data classified as Bots(1) and who are determined as Human Users (0)}
         \PY{n}{labelled\PYZus{}df}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}83}]:}          user\_id
         bot\_flg         
         0           1747
         1            826
\end{Verbatim}
            
    \textbf{Extract Tweet data from Twitter developer API for twitter users
in the pre-labelled data set}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}84}]:} \PY{c+c1}{\PYZsh{} Method Implementation to extract tweets for a set of users}
         \PY{k}{def} \PY{n+nf}{extract\PYZus{}tweets\PYZus{}for\PYZus{}users}\PY{p}{(}\PY{n}{labelled\PYZus{}users}\PY{p}{,} \PY{n}{target\PYZus{}location}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Extract upto 3200 tweets for the given set of labelled users}
         \PY{l+s+sd}{        and store the tweets for each individual user in \PYZlt{}user\PYZus{}id\PYZgt{}.data file in the target\PYZus{}location}
         \PY{l+s+sd}{        stats of success\PYZus{}users for whom data was retreived successfully and restricted\PYZus{}users for }
         \PY{l+s+sd}{        whom data retreival was not successful are also written out to the target location}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{       Args:}
         \PY{l+s+sd}{           labelled\PYZus{}users: Datframe containing set of labelled users with twitter\PYZus{}user\PYZus{}id}
         \PY{l+s+sd}{           target\PYZus{}location: Location where the data files with extracted tweets should be written}
         \PY{l+s+sd}{                  }
         \PY{l+s+sd}{       Returns:}
         \PY{l+s+sd}{           success\PYZus{}users: Dataframe containining list of successful users for whom data was extracted}
         \PY{l+s+sd}{           restricted\PYZus{}users: Dataframe containining list of restricted users}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}  
         
             \PY{c+c1}{\PYZsh{} Instantiate a Dataframe to store the restricted users whose information cannot be accessed}
             \PY{n}{restricted\PYZus{}users} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reason}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             \PY{n}{success\PYZus{}users} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Loop through each user in labelled\PYZus{}users and write the json extracts into \PYZsq{}\PYZlt{}user\PYZus{}id\PYZgt{}.dat\PYZsq{} file}
             \PY{n}{max\PYZus{}tweet\PYZus{}cnt} \PY{o}{=} \PY{l+m+mi}{3000}
             \PY{n}{stop\PYZus{}process} \PY{o}{=} \PY{k+kc}{False}
             \PY{k}{for} \PY{n}{index}\PY{p}{,} \PY{n}{row} \PY{o+ow}{in} \PY{n}{labelled\PYZus{}users}\PY{o}{.}\PY{n}{iterrows}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                 \PY{n}{user\PYZus{}id} \PY{o}{=} \PY{n}{row}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
                 \PY{n}{curr\PYZus{}id} \PY{o}{=} \PY{k+kc}{None} \PY{c+c1}{\PYZsh{} Latest tweet ID retreived for the current user}
         
                 \PY{c+c1}{\PYZsh{} Check if rate limit has reached by checking stop\PYZus{}process flag before proceeding}
                 \PY{k}{if} \PY{n}{stop\PYZus{}process} \PY{o}{==} \PY{k+kc}{True}\PY{p}{:}
                     \PY{k}{break}
         
                 \PY{c+c1}{\PYZsh{} Open Write handle for \PYZsq{}\PYZlt{}user\PYZus{}id\PYZgt{}.dat\PYZsq{} file to write the extracted json feeds}
                 \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{target\PYZus{}location} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{user\PYZus{}id}\PY{p}{)} \PY{o}{+} \PY{n}{DATA\PYZus{}FILE\PYZus{}SUFFIX}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
         
                     \PY{k}{try}\PY{p}{:}
                         \PY{c+c1}{\PYZsh{} Initiate remaining count to max count}
                         \PY{n}{remaining\PYZus{}cnt} \PY{o}{=} \PY{n}{max\PYZus{}tweet\PYZus{}cnt}
         
                         \PY{c+c1}{\PYZsh{} Check if still max count has not reached by checking remaining count}
                         \PY{k}{while} \PY{n}{remaining\PYZus{}cnt} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{:}
         
                             \PY{n}{tweet\PYZus{}cnt} \PY{o}{=} \PY{l+m+mi}{0}
                             \PY{n}{tweet\PYZus{}extracts} \PY{o}{=} \PY{k+kc}{None}
                             \PY{c+c1}{\PYZsh{} Check if tweets have been retreived for user in this session}
                             \PY{c+c1}{\PYZsh{} by checking the id of the last tweet retreived}
                             \PY{k}{if} \PY{n}{curr\PYZus{}id} \PY{o}{==} \PY{k+kc}{None}\PY{p}{:}
                                 \PY{n}{tweet\PYZus{}extracts} \PY{o}{=} \PY{n}{get\PYZus{}all\PYZus{}tweets\PYZus{}json}\PY{p}{(}\PY{n}{user\PYZus{}id}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}
                             \PY{k}{else}\PY{p}{:}
                                 \PY{n}{tweet\PYZus{}extracts} \PY{o}{=} \PY{n}{get\PYZus{}all\PYZus{}tweets\PYZus{}json}\PY{p}{(}\PY{n}{user\PYZus{}id}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{,} \PY{n}{curr\PYZus{}id}\PY{p}{)}
         
                             \PY{c+c1}{\PYZsh{} Encode each json tweet retreived and write to the user dat file}
                             \PY{k}{for} \PY{n}{tweet} \PY{o+ow}{in} \PY{n}{tweet\PYZus{}extracts}\PY{p}{:}
                                 \PY{n}{curr\PYZus{}id} \PY{o}{=} \PY{n}{tweet}\PY{o}{.}\PY{n}{\PYZus{}json}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}
                                 \PY{n}{f}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{jsonpickle}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{tweet}\PY{o}{.}\PY{n}{\PYZus{}json}\PY{p}{,} \PY{n}{unpicklable}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)} \PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                                 \PY{n}{tweet\PYZus{}cnt} \PY{o}{=} \PY{n}{tweet\PYZus{}cnt} \PY{o}{+} \PY{l+m+mi}{1}
         
                             \PY{c+c1}{\PYZsh{} Update remaining count to reach max limit}
                             \PY{n}{remaining\PYZus{}cnt} \PY{o}{=} \PY{n}{max\PYZus{}tweet\PYZus{}cnt} \PY{o}{\PYZhy{}} \PY{n}{tweet\PYZus{}cnt}
         
                             \PY{c+c1}{\PYZsh{} If no tweets was available to retreive then move to next user}
                             \PY{k}{if} \PY{n}{tweet\PYZus{}cnt} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                                 \PY{k}{break}\PY{p}{;}
         
                         \PY{c+c1}{\PYZsh{} Append stats of current users to success users dataframe}
                         \PY{n}{success\PYZus{}users} \PY{o}{=} \PY{n}{success\PYZus{}users}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{user\PYZus{}id}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{row}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{\PYZcb{}}
                                                              \PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
                     \PY{k}{except} \PY{n}{tweepy}\PY{o}{.}\PY{n}{TweepError} \PY{k}{as} \PY{n}{e}\PY{p}{:}
                         \PY{c+c1}{\PYZsh{} Handle TweepError encountered during extraction and update restricted\PYZus{}users dataframe}
                         \PY{n}{restricted\PYZus{}users} \PY{o}{=} \PY{n}{restricted\PYZus{}users}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{user\PYZus{}id}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{row}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
                                                                     \PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reason}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{e}\PY{o}{.}\PY{n}{reason}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
                     \PY{k}{except} \PY{n}{tweepy}\PY{o}{.}\PY{n}{RateLimitError} \PY{k}{as} \PY{n}{rate\PYZus{}err}\PY{p}{:}
                         \PY{c+c1}{\PYZsh{} Handle limit reached error and mark to stop retreival process}
                         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Tweet Data Extraction terminated due to RATE LIMIT ERROR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Current User ID \PYZhy{} }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n}{user\PYZus{}id}\PY{p}{)}
                         \PY{n}{stop\PYZus{}process} \PY{o}{=} \PY{k+kc}{True}
         
                 \PY{c+c1}{\PYZsh{} Close user data file handle}
                 \PY{n}{f}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Write Success User and Restricted user stats to CSV file}
             \PY{n}{success\PYZus{}users}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{n}{target\PYZus{}location} \PY{o}{+} \PY{n}{SUCCESS\PYZus{}USER\PYZus{}STATS}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
             \PY{n}{restricted\PYZus{}users}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{n}{target\PYZus{}location} \PY{o}{+} \PY{n}{RESTRICTED\PYZus{}USER\PYZus{}STATS}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{success\PYZus{}users}\PY{p}{,} \PY{n}{restricted\PYZus{}users}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} \PY{c+c1}{\PYZsh{} MARKED FOR EXECUTION AS NEEDED \PYZhy{} NOTE \PYZhy{} EXTRACTION TAKES A LONG TIME}
         \PY{c+c1}{\PYZsh{} Download the latest 3000 tweets as json for each user in varol\PYZhy{}2017.dat (labelled\PYZus{}df)}
         
         \PY{c+c1}{\PYZsh{} success\PYZus{}users, restricted\PYZus{}users = extract\PYZus{}tweets\PYZus{}for\PYZus{}users(labelled\PYZus{}df, TWEET\PYZus{}EXTRACT\PYZus{}LOCATION\PYZus{}TRAIN)}
\end{Verbatim}


    Data Preprocessing

    \textbf{Utility methods to help in Data Processing}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}127}]:} \PY{c+c1}{\PYZsh{} Lambda function to check if an Object is NULL and return 0 if yes and 1 if not null}
          \PY{n}{check\PYZus{}if\PYZus{}null} \PY{o}{=} \PY{k}{lambda} \PY{n}{obj}\PY{p}{:} \PY{l+m+mi}{0} \PY{k}{if} \PY{n}{obj} \PY{o}{==} \PY{k+kc}{None} \PY{k}{else} \PY{l+m+mi}{1}
          
          \PY{c+c1}{\PYZsh{} Lambda function to Binary code a boolean value \PYZhy{} True to 1 and False to 0}
          \PY{n}{binary\PYZus{}code\PYZus{}boolean} \PY{o}{=} \PY{k}{lambda} \PY{n}{boolean}\PY{p}{:} \PY{l+m+mi}{1} \PY{k}{if} \PY{n}{boolean} \PY{o}{==} \PY{k+kc}{True} \PY{k}{else} \PY{l+m+mi}{0}
          
          \PY{c+c1}{\PYZsh{} Lambda function to get count of a list}
          \PY{n}{get\PYZus{}count} \PY{o}{=} \PY{k}{lambda} \PY{n}{array}\PY{p}{:} \PY{l+m+mi}{0} \PY{k}{if} \PY{p}{(}\PY{n}{array} \PY{o}{==} \PY{k+kc}{None} \PY{o+ow}{or} \PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{array}\PY{p}{)} \PY{o+ow}{is} \PY{o+ow}{not} \PY{n+nb}{list}\PY{p}{)}\PY{p}{)} \PY{k}{else} \PY{n+nb}{len}\PY{p}{(}\PY{n}{array}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Utility function to separate hashtag topics from the given string phrase}
          \PY{k}{def} \PY{n+nf}{extract\PYZus{}hash\PYZus{}tags}\PY{p}{(}\PY{n}{s}\PY{p}{)}\PY{p}{:}
              \PY{k}{return} \PY{n+nb}{set}\PY{p}{(}\PY{n}{part}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{k}{for} \PY{n}{part} \PY{o+ow}{in} \PY{n}{s}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)} \PY{k}{if} \PY{n}{part}\PY{o}{.}\PY{n}{startswith}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Utility method to Clean the provided text content}
          \PY{k}{def} \PY{n+nf}{cleanTagText}\PY{p}{(}\PY{n}{text}\PY{p}{)}\PY{p}{:}
              \PY{n}{text} \PY{o}{=} \PY{n+nb}{str}\PY{p}{(}\PY{n}{text}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{encoding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ascii}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{errors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{replace}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
              \PY{n}{text} \PY{o}{=} \PY{n}{text}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
              \PY{n}{text} \PY{o}{=} \PY{n}{text}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{?}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZti{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
              \PY{n}{text} \PY{o}{=} \PY{n}{text}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}
              \PY{k}{return} \PY{n}{text}
\end{Verbatim}


    \textbf{Parse the JSON feeds of the tweets received and create a
flattened data frame with extracted and preprocessed attributes}

\textbf{Cleanse, modify and update attributes extracted from the tweet
feeds}

    Data Cleansing and Standardisation

\textbf{Parse the JSON feeds of the tweets received and create a
flattened data frame with extracted and preprocessed attributes}\\
\textbf{Cleanse, modify and update attributes extracted from the tweet
feeds}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}128}]:} \PY{c+c1}{\PYZsh{} Method Implementation to parse and extract features from tweets posted by a specified user}
          \PY{c+c1}{\PYZsh{} Approx 14 User level features and 17 Tweet level features are extracted from the Tweet objects}
          
          \PY{k}{def} \PY{n+nf}{parse\PYZus{}and\PYZus{}extract\PYZus{}tweets\PYZus{}for\PYZus{}user}\PY{p}{(}\PY{n}{user\PYZus{}id}\PY{p}{,} \PY{n}{source\PYZus{}location}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Parse the JSON tweets for the given user id and extract the features.}
          \PY{l+s+sd}{        Features extracted will be user level features and tweet level features}
          \PY{l+s+sd}{        }
          \PY{l+s+sd}{       Args:}
          \PY{l+s+sd}{           user\PYZus{}id: Twitter User ID of an user as recognized by Twitter developer API.}
          \PY{l+s+sd}{           source\PYZus{}location: Location of the dat files from which tweets are to be extracted}
          \PY{l+s+sd}{       Returns:}
          \PY{l+s+sd}{           user\PYZus{}features\PYZus{}df: Dataframe containining user level features for the given user}
          \PY{l+s+sd}{           tweet\PYZus{}features\PYZus{}df: Dataframe of tweets containing features extracted from individual tweets for user}
          \PY{l+s+sd}{           }
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}  
              \PY{n}{user\PYZus{}features\PYZus{}df} \PY{o}{=} \PY{k+kc}{None}
              \PY{n}{tweets\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{p}{]}
              \PY{n}{has\PYZus{}user\PYZus{}data} \PY{o}{=} \PY{k+kc}{False}
              
              \PY{c+c1}{\PYZsh{} Parse the JSON Feed file with tweets obtained from Twitter for the given User ID}
              \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{source\PYZus{}location} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{user\PYZus{}id}\PY{p}{)} \PY{o}{+} \PY{n}{DATA\PYZus{}FILE\PYZus{}SUFFIX}\PY{p}{)} \PY{k}{as} \PY{n}{fjson}\PY{p}{:}
                  \PY{n}{tweet\PYZus{}jsons} \PY{o}{=} \PY{n}{fjson}\PY{o}{.}\PY{n}{readlines}\PY{p}{(}\PY{p}{)}
              \PY{n}{fjson}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} Loop through all tweets and extract tweet level features for each tweet}
              \PY{c+c1}{\PYZsh{} Extract User level features from the tweet object just for the first tweet}
              \PY{k}{for} \PY{n}{current\PYZus{}tweet} \PY{o+ow}{in} \PY{n}{tweet\PYZus{}jsons}\PY{p}{:}
          
                  \PY{c+c1}{\PYZsh{} Load the tweet object into a dictionary by decoding the JSON string}
                  \PY{n}{tweet\PYZus{}dict} \PY{o}{=} \PY{n}{json}\PY{o}{.}\PY{n}{loads}\PY{p}{(}\PY{n}{current\PYZus{}tweet}\PY{p}{)}
                  
                  \PY{c+c1}{\PYZsh{} Extract User level features just from the first tweet}
                  \PY{c+c1}{\PYZsh{} All Boolean attributes are binary encoded to 1 (True) or 0 (False)}
                  \PY{k}{if} \PY{n}{has\PYZus{}user\PYZus{}data} \PY{o}{==} \PY{k+kc}{False}\PY{p}{:}
                      \PY{n}{usr\PYZus{}features} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
                      
                      \PY{c+c1}{\PYZsh{} Get the user object as dictionary from the tweet}
                      \PY{n}{user\PYZus{}dict} \PY{o}{=} \PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}\PY{p}{)}
                      
                      \PY{c+c1}{\PYZsh{} Get ID of the user (Number)}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                      
                      \PY{c+c1}{\PYZsh{} Boolean attribute \PYZhy{} True indicates user has not altered default profile}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{default\PYZus{}profile}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{binary\PYZus{}code\PYZus{}boolean}\PY{p}{(}\PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{default\PYZus{}profile}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                      \PY{c+c1}{\PYZsh{} Boolean attribute \PYZhy{} True indicates user has not altered default profile Image}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{default\PYZus{}profile\PYZus{}image}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{binary\PYZus{}code\PYZus{}boolean}\PY{p}{(}\PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{default\PYZus{}profile\PYZus{}image}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
          
                      \PY{c+c1}{\PYZsh{} Int attribute \PYZhy{} Number of tweets user has liked in lifetime of account}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{favourites\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{favourites\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                      \PY{c+c1}{\PYZsh{} Int attribute \PYZhy{} Number of current followers for this account}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{followers\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{followers\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                      \PY{c+c1}{\PYZsh{} Int attribute \PYZhy{} Number of users being followed by this account}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{friends\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{friends\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                      \PY{c+c1}{\PYZsh{} Int attribute \PYZhy{} Number of public lists that this user is a member of}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{listed\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{listed\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                      \PY{c+c1}{\PYZsh{} Int attribute \PYZhy{} Number of tweets (including retweets) issued by this user}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{statuses\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{statuses\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
                      \PY{c+c1}{\PYZsh{} Boolean attribute \PYZhy{} True \PYZhy{} Indicates user has enabled geo tagging of their tweets}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{geo\PYZus{}enabled}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{binary\PYZus{}code\PYZus{}boolean}\PY{p}{(}\PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{geo\PYZus{}enabled}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                      \PY{c+c1}{\PYZsh{} Boolean attribute \PYZhy{} True \PYZhy{} Indicates user has chosen to protect their tweets}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{protected}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{binary\PYZus{}code\PYZus{}boolean}\PY{p}{(}\PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{protected}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                      \PY{c+c1}{\PYZsh{} Boolean attribute \PYZhy{} True \PYZhy{} Indicates user has a verified account}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{verified}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{binary\PYZus{}code\PYZus{}boolean}\PY{p}{(}\PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{verified}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                                  
                      \PY{c+c1}{\PYZsh{} Twitter Internal attributes}
                      \PY{c+c1}{\PYZsh{} Boolean attribute \PYZhy{} True \PYZhy{} Indicates user has an extended profile}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{has\PYZus{}extended\PYZus{}profile}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{binary\PYZus{}code\PYZus{}boolean}\PY{p}{(}\PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{has\PYZus{}extended\PYZus{}profile}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                      \PY{c+c1}{\PYZsh{} Boolean attribute \PYZhy{} True \PYZhy{} Indicates if the user is a participant of Translator community }
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}translator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{binary\PYZus{}code\PYZus{}boolean}\PY{p}{(}\PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}translator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                      
                      \PY{c+c1}{\PYZsh{} String attribute \PYZhy{} Denotes the code of the language set for the user account}
                      \PY{n}{usr\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}lang}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{user\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lang}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                      
                      \PY{c+c1}{\PYZsh{} Create a dataframe for the user features}
                      \PY{n}{user\PYZus{}features\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{usr\PYZus{}features}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                      \PY{n}{has\PYZus{}user\PYZus{}data} \PY{o}{=} \PY{k+kc}{True}
              
                  \PY{c+c1}{\PYZsh{} Extract Tweet Level Features to be consolidated and grouped to user level during data preparation}
                  \PY{n}{tweet\PYZus{}features} \PY{o}{=}\PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{user\PYZus{}id}
                  
                  \PY{c+c1}{\PYZsh{} Get ID of the Tweet (Number)}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tweet\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              
                  \PY{c+c1}{\PYZsh{} Perform SENTIMENT ANALYSIS on the text content of the tweet}
                  \PY{n}{sentiment\PYZus{}score} \PY{o}{=} \PY{n}{TextBlob}\PY{p}{(}\PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{sentiment}
                  \PY{c+c1}{\PYZsh{} Calculate Polarity (Level of emotion) score (Range 0\PYZhy{}1) from sentiment analysis }
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{senti\PYZus{}polarity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{sentiment\PYZus{}score}\PY{o}{.}\PY{n}{polarity}
                  \PY{c+c1}{\PYZsh{} Calculate Subjectivity score (Range 0\PYZhy{}1) from sentiment analysis}
                  \PY{c+c1}{\PYZsh{} 0 is very Objective and 1 is very subjective}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{senti\PYZus{}subjectivity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{sentiment\PYZus{}score}\PY{o}{.}\PY{n}{subjectivity}
              
                  \PY{c+c1}{\PYZsh{} String attribute \PYZhy{} Indicates date and timestamp when the tweet was created}
                  \PY{c+c1}{\PYZsh{} Convert to Date/Time object}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{created\PYZus{}at}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{created\PYZus{}at}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             
                  \PY{c+c1}{\PYZsh{} Int attribute \PYZhy{} Indicates number of times this tweet has been liked by users}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{favorite\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{favorite\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} Int attribute \PYZhy{} Indicates number of times this tweet has been retweeted by users}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{retweet\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{retweet\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              
                  \PY{c+c1}{\PYZsh{} Boolean attribute \PYZhy{} True Indicates that tweet is associated with a place}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{place\PYZus{}reference}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{check\PYZus{}if\PYZus{}null}\PY{p}{(}\PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{place}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}**}
                  \PY{c+c1}{\PYZsh{} Boolean attribute \PYZhy{} True Indicates that tweet contains possibly sensitive content}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{possibly\PYZus{}sensitive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{binary\PYZus{}code\PYZus{}boolean}\PY{p}{(}\PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{possibly\PYZus{}sensitive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}**}
                  \PY{c+c1}{\PYZsh{} Boolean attribute \PYZhy{} True Indicates that tweet is a reply to an existing tweet}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}a\PYZus{}reply}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{check\PYZus{}if\PYZus{}null}\PY{p}{(}\PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{in\PYZus{}reply\PYZus{}to\PYZus{}status\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}**}
                  \PY{c+c1}{\PYZsh{} Boolean attribute \PYZhy{} True Indicates that this tweet is a Quoted tweet}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}quote\PYZus{}status}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{binary\PYZus{}code\PYZus{}boolean}\PY{p}{(}\PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}quote\PYZus{}status}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}**}
                  \PY{c+c1}{\PYZsh{} Boolean attribute \PYZhy{} True Indicates that this tweet is a retweet of an existing tweet}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}a\PYZus{}retweet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{check\PYZus{}if\PYZus{}null}\PY{p}{(}\PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{retweeted\PYZus{}status}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}**}
              
                  \PY{c+c1}{\PYZsh{} Extract Info on Media and external entitities associated with the tweet}
                  \PY{n}{entities\PYZus{}dict} \PY{o}{=} \PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entities}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}\PY{p}{)} \PY{c+c1}{\PYZsh{}**}
                  \PY{c+c1}{\PYZsh{} Int attribute \PYZhy{} Indicates number of Hash tags referred to in the tweet}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{get\PYZus{}count}\PY{p}{(}\PY{n}{entities\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hashtags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} Int attribute \PYZhy{} Indicates number of media content attached with the tweet}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{media\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{get\PYZus{}count}\PY{p}{(}\PY{n}{entities\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{media}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} Int attribute \PYZhy{} Indicates number of URLs and links embedded in the tweet}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{url\PYZus{}ref\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{get\PYZus{}count}\PY{p}{(}\PY{n}{entities\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{urls}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} Int attribute \PYZhy{} Indicates number of symbols embedded in the tweet}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{symbols\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{get\PYZus{}count}\PY{p}{(}\PY{n}{entities\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{symbols}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} Int attribute \PYZhy{} Indicates number of User references in the tweet}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}ref\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{get\PYZus{}count}\PY{p}{(}\PY{n}{entities\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}mentions}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                  
                  \PY{c+c1}{\PYZsh{} Extract Hash tags from the tweet text to perform topic modeling}
                  \PY{n}{hash\PYZus{}tags} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}
                  \PY{k}{if} \PY{n}{get\PYZus{}count}\PY{p}{(}\PY{n}{entities\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hashtags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{:}
                      \PY{n}{hash\PYZus{}tags} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{filter}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,}\PY{n}{extract\PYZus{}hash\PYZus{}tags}\PY{p}{(}\PY{n}{cleanTagText}\PY{p}{(}\PY{n}{tweet\PYZus{}dict}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                  \PY{n}{tweet\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{hash\PYZus{}tags}
                  
                  \PY{c+c1}{\PYZsh{} Add the tweet to the list of user tweets parsed}
                  \PY{n}{tweets\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{tweet\PYZus{}features}\PY{p}{)}
                  
              \PY{c+c1}{\PYZsh{} Create a dataframe for the tweet features}
              \PY{n}{tweet\PYZus{}features\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{tweets\PYZus{}list}\PY{p}{)} 
                  
              \PY{k}{return} \PY{n}{user\PYZus{}features\PYZus{}df}\PY{p}{,} \PY{n}{tweet\PYZus{}features\PYZus{}df}
\end{Verbatim}


    Feature Extraction

\textbf{Extract, Derive and Create new features from the attributes
extracted from the tweet feeds}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}88}]:} \PY{c+c1}{\PYZsh{} Method implementation to consolidate, merge tweet level stats gathered to user level}
         \PY{c+c1}{\PYZsh{} Stats are merged based on the number of tweets collected for the user. Usually \PYZti{}3000\PYZhy{}3200}
         
         \PY{k}{def} \PY{n+nf}{Merge\PYZus{}tweet\PYZus{}stats\PYZus{}to\PYZus{}user\PYZus{}level}\PY{p}{(}\PY{n}{user\PYZus{}id}\PY{p}{,} \PY{n}{tweet\PYZus{}cnt}\PY{p}{,} \PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Parse the JSON tweets for the given user id and extract the features.}
         \PY{l+s+sd}{        Features extracted will be user level features and tweet level features}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{       Args:}
         \PY{l+s+sd}{           user\PYZus{}id: Twitter User ID of an user as recognized by Twitter developer API.}
         \PY{l+s+sd}{           tweet\PYZus{}cnt: Count of tweets extracted for the user}
         \PY{l+s+sd}{           tweet\PYZus{}features\PYZus{}df: Dataframe of tweets containing features extracted from individual tweets for user}
         \PY{l+s+sd}{       Returns:}
         \PY{l+s+sd}{           tweet\PYZus{}merged\PYZus{}df: Dataframe for tweet level features consolidated to user level}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}} 
             \PY{n}{merged\PYZus{}stats} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
             
             \PY{c+c1}{\PYZsh{} Calculate Mean of Sentiment analysis scores across all messages created by user}
             \PY{c+c1}{\PYZsh{} Value range is between 0 and 1}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{senti\PYZus{}polarity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{senti\PYZus{}polarity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{senti\PYZus{}subjectivity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{senti\PYZus{}subjectivity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Calculate Mean stats of all binary value (0 or 1) features}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{place\PYZus{}reference}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{place\PYZus{}reference}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{possibly\PYZus{}sensitive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{possibly\PYZus{}sensitive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}a\PYZus{}reply}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}a\PYZus{}reply}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}quote\PYZus{}status}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}quote\PYZus{}status}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}a\PYZus{}retweet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}a\PYZus{}retweet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Calculate Mean stats of all integer value features}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{favorite\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{favorite\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{retweet\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{retweet\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{media\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{media\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{url\PYZus{}ref\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{url\PYZus{}ref\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{symbols\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{symbols\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}ref\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}ref\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
           
             \PY{c+c1}{\PYZsh{} Calculate the Tweeting frequency in Hours based on Creation Time Interval and total count of tweets by user}
             \PY{n}{creation\PYZus{}intrval} \PY{o}{=} \PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{created\PYZus{}at}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{created\PYZus{}at}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tweet\PYZus{}freq\PYZus{}hrs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{p}{(}\PY{n}{creation\PYZus{}intrval}\PY{o}{.}\PY{n}{days}\PY{o}{*}\PY{l+m+mi}{24} \PY{o}{+} 
                                                     \PY{n}{creation\PYZus{}intrval}\PY{o}{.}\PY{n}{seconds}\PY{o}{/}\PY{o}{/}\PY{l+m+mi}{3600}\PY{p}{)} \PY{o}{/} \PY{n+nb}{int}\PY{p}{(}\PY{n}{tweet\PYZus{}cnt}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Consolidate all hash tags from all user messages into 1 string}
             \PY{n}{hash\PYZus{}tags} \PY{o}{=} \PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{n}{sep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}str(tweet\PYZus{}features\PYZus{}df[\PYZsq{}hash\PYZus{}tags\PYZsq{}].apply(\PYZsq{} \PYZsq{}.join)).strip()}
             \PY{k}{if} \PY{n}{hash\PYZus{}tags} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                 \PY{n}{hash\PYZus{}tags} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{none}\PY{l+s+s1}{\PYZsq{}}
             \PY{n}{merged\PYZus{}stats}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{hash\PYZus{}tags}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Create a Dataframe with the Merged stats}
             \PY{n}{tweet\PYZus{}merged\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{merged\PYZus{}stats}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{tweet\PYZus{}merged\PYZus{}df}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} \PY{k}{def} \PY{n+nf}{consolidate\PYZus{}features\PYZus{}at\PYZus{}user\PYZus{}level}\PY{p}{(}\PY{n}{source\PYZus{}location}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Extract, parse and Consolidate all Features for each user in the extracted data set}
         \PY{l+s+sd}{       }
         \PY{l+s+sd}{       Args:}
         \PY{l+s+sd}{           source\PYZus{}location: Location of the dat files from which tweets are to be extracted}
         \PY{l+s+sd}{       Returns:}
         \PY{l+s+sd}{           consolidated\PYZus{}ftrs\PYZus{}df: Dataframe with consolidated features for all users}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}} 
             \PY{c+c1}{\PYZsh{} Create a Dataframe with users for whom data was extracted successfully}
             \PY{n}{success\PYZus{}users\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{source\PYZus{}location} \PY{o}{+} \PY{n}{SUCCESS\PYZus{}USER\PYZus{}STATS}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} For each user.. Parse, extract features on user and tweet level.. then merge tweet level features to user level}
             \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}
             \PY{k}{for} \PY{n}{index}\PY{p}{,} \PY{n}{row} \PY{o+ow}{in} \PY{n}{success\PYZus{}users\PYZus{}df}\PY{o}{.}\PY{n}{iterrows}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Parse, extract features on user and tweet level}
                 \PY{n}{user\PYZus{}features\PYZus{}df}\PY{p}{,} \PY{n}{tweet\PYZus{}features\PYZus{}df} \PY{o}{=} \PY{n}{parse\PYZus{}and\PYZus{}extract\PYZus{}tweets\PYZus{}for\PYZus{}user}\PY{p}{(}\PY{n}{row}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{source\PYZus{}location}\PY{p}{)}   
                 \PY{n}{tweet\PYZus{}count} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
                 \PY{k}{if} \PY{n}{tweet\PYZus{}count} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} Merge tweet level features to user level based on aggregation}
                     \PY{n}{tweet\PYZus{}merged\PYZus{}df} \PY{o}{=} \PY{n}{Merge\PYZus{}tweet\PYZus{}stats\PYZus{}to\PYZus{}user\PYZus{}level}\PY{p}{(}\PY{n}{row}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{tweet\PYZus{}count}\PY{p}{,} \PY{n}{tweet\PYZus{}features\PYZus{}df}\PY{p}{)}
         
                     \PY{c+c1}{\PYZsh{} Merge the user and aggregated tweet features and Concat to a consolidated Dataframe}
                     \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{user\PYZus{}features\PYZus{}df}\PY{p}{,} \PY{n}{tweet\PYZus{}merged\PYZus{}df}\PY{p}{]}
                                                                                  \PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{;}
         
             \PY{c+c1}{\PYZsh{} Merge the consolidated features with the pre classification labels}
             \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{success\PYZus{}users\PYZus{}df}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{on}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Just In Case \PYZhy{} Drop NaN and Null rows}
             \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}    
         
             \PY{c+c1}{\PYZsh{} Write the Consolidated features to the source location for later retreival}
             \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{n}{source\PYZus{}location} \PY{o}{+} \PY{n}{CONSOLIDATED\PYZus{}FEATURES}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{TWEET\PYZus{}EXTRACT\PYZus{}LOCATION\PYZus{}TRAIN} \PY{o}{+} \PY{n}{CONSOLIDATED\PYZus{}FEATURES}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} MARKED FOR EXECUTION AS NEEDED \PYZhy{} NOTE \PYZhy{} PARSING and CONSOLIDATES TAKES A LONG TIME}
         \PY{c+c1}{\PYZsh{} Extract and consolidate all features for every user}
         \PY{c+c1}{\PYZsh{} consolidated\PYZus{}ftrs\PYZus{}df = consolidate\PYZus{}features\PYZus{}at\PYZus{}user\PYZus{}level(TWEET\PYZus{}EXTRACT\PYZus{}LOCATION\PYZus{}TRAIN)}
         
         \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}90}]:}             user\_id  default\_profile  default\_profile\_image  favourites\_count  followers\_count  friends\_count  listed\_count  statuses\_count  geo\_enabled  protected     verified  has\_extended\_profile  is\_translator  senti\_polarity  senti\_subjectivity  place\_reference  possibly\_sensitive   is\_a\_reply  is\_quote\_status  is\_a\_retweet  favorite\_count  retweet\_count  hash\_tags\_cnt    media\_cnt  url\_ref\_cnt  symbols\_cnt  user\_ref\_cnt  tweet\_freq\_hrs      bot\_flg
         count  2.139000e+03      2139.000000            2139.000000       2139.000000      2139.000000    2139.000000   2139.000000     2139.000000  2139.000000     2139.0  2139.000000           2139.000000         2139.0     2139.000000         2139.000000      2139.000000         2139.000000  2139.000000      2139.000000   2139.000000     2139.000000    2139.000000    2139.000000  2139.000000  2139.000000  2139.000000   2139.000000     2139.000000  2139.000000
         mean   1.364801e+09         0.421225               0.039271       6027.263675      1679.877513     795.085554     31.751286    13783.351566     0.495091        0.0     0.011688              0.407200            0.0        0.099820            0.275848         0.031882            0.006391     0.126498         0.056089      0.341183        0.883115    2914.856304       0.415956     0.176507     0.409275     0.000877      0.676613       20.980187     0.328191
         std    1.303371e+09         0.493871               0.194284      14488.054908      8198.758782    1960.674949    150.966719    25192.829540     0.500093        0.0     0.107501              0.491428            0.0        0.083318            0.114966         0.095798            0.063096     0.164704         0.081115      0.325837        3.621058    7006.615150       0.616179     0.193152     0.356946     0.010571      0.484800      100.018217     0.469664
         min    1.325300e+04         0.000000               0.000000          0.000000         0.000000       0.000000      0.000000        1.000000     0.000000        0.0     0.000000              0.000000            0.0       -0.743000            0.000000         0.000000            0.000000     0.000000         0.000000      0.000000        0.000000       0.000000       0.000000     0.000000     0.000000     0.000000      0.000000        0.000000     0.000000
         25\%    2.365333e+08         0.000000               0.000000         94.000000       125.500000     133.000000      1.000000     2242.000000     0.000000        0.0     0.000000              0.000000            0.0        0.049000            0.226500         0.000000            0.000000     0.003000         0.002000      0.050000        0.034500       8.499000       0.048000     0.050000     0.122000     0.000000      0.315500        4.902000     0.000000
         50\%    7.511447e+08         0.000000               0.000000       1325.000000       347.000000     343.000000      4.000000     5561.000000     0.000000        0.0     0.000000              0.000000            0.0        0.086000            0.300000         0.000000            0.000000     0.058000         0.022000      0.242000        0.181000     441.084000       0.159000     0.122000     0.258000     0.000000      0.663000       10.175000     0.000000
         75\%    2.582829e+09         1.000000               0.000000       5977.500000       847.000000     785.000000     16.000000    14295.500000     1.000000        0.0     0.000000              1.000000            0.0        0.137000            0.344000         0.005000            0.000000     0.192000         0.076500      0.577500        0.636500    2477.525000       0.566000     0.233000     0.723500     0.000000      0.987500       17.408500     1.000000
         max    4.627817e+09         1.000000               1.000000     289192.000000    247923.000000   35509.000000   4569.000000   338380.000000     1.000000        0.0     1.000000              1.000000            0.0        0.781000            0.993000         1.000000            1.000000     1.000000         0.645000      1.000000       80.720000  120360.862000       5.422000     1.000000     2.000000     0.369000      5.038000     3619.000000     1.000000
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{c+c1}{\PYZsh{} Preview the consolidated features}
         \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}91}]:}       user\_id  default\_profile  default\_profile\_image  favourites\_count  followers\_count  friends\_count  listed\_count  statuses\_count  geo\_enabled  protected  verified  has\_extended\_profile  is\_translator user\_lang  senti\_polarity  senti\_subjectivity  place\_reference  possibly\_sensitive  is\_a\_reply  is\_quote\_status  is\_a\_retweet  favorite\_count  retweet\_count  hash\_tags\_cnt  media\_cnt  url\_ref\_cnt  symbols\_cnt  user\_ref\_cnt  tweet\_freq\_hrs                                          hash\_tags  bot\_flg
         0    44324787                1                      0             15011               11            141             1           14809            0          0         0                     0              0        ko           0.011               0.043              0.0                 0.0       0.002            0.001         0.998           0.000       1281.749          3.389      0.892        0.229          0.0         1.024           1.897  twice1starenatour sonchaeyoung fes2018 twice n{\ldots}        0
         1  3098421349                1                      0                88              273            912             5            2263            0          0         0                     0              0        en           0.196               0.347              0.0                 0.0       0.000            0.003         0.023           0.270          0.269          0.601      0.444        0.619          0.0         0.028          14.288  hilarious petodor lenses bedrooms arearugs tip{\ldots}        1
         2   554067867                1                      0              2707               28           1586            18            5357            0          0         0                     0              0        en           0.134               0.304              0.0                 0.0       0.000            0.038         1.000           0.000        372.719          1.159      0.146        0.440          0.0         1.138           6.695  momentofclarity makeyourownlane cw3 prettyeyes{\ldots}        1
         3   256597786                1                      0                10              162            482             3            1206            0          0         0                     0              0        en           0.283               0.335              0.0                 0.0       0.011            0.040         0.594           0.037       1757.059          1.048      0.056        0.412          0.0         1.279          57.982  thefifthwatches lilyjames! vudu esuranceswe pr{\ldots}        1
         4   103351486                0                      0                55              182            145             4            6595            0          0         0                     0              0        en           0.015               0.130              0.0                 0.0       0.006            0.000         0.008           0.017        112.968          0.007      0.002        0.948          0.0         0.019           8.089  trump2016 throwbackthursday deplorable america{\ldots}        1
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} \PY{c+c1}{\PYZsh{} Drop user\PYZus{}id from the dataframe as that is not needed for classification}
         \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{user\PYZus{}id}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \section{Exploratory Data Analysis on the extracted features and
attributes}\label{exploratory-data-analysis-on-the-extracted-features-and-attributes}

    Matrix plot of User Level count features across Bot and Human classes

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}93}]:} \PY{c+c1}{\PYZsh{} Separate Bot and Human data into separate frames for analysis}
         \PY{n}{df\PYZus{}bot} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{p}{[}\PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{bot\PYZus{}flg}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{n}{df\PYZus{}human} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{p}{[}\PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{bot\PYZus{}flg}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}94}]:} \PY{c+c1}{\PYZsh{} Create a Matrix plot across Bot and Human classes}
         \PY{n}{ax} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{p}{,} \PY{n}{hue}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{bot\PYZus{}flg}\PY{l+s+s2}{\PYZdq{}}
                            \PY{p}{,} \PY{n+nb}{vars}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{favourites\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{followers\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{friends\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{listed\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{statuses\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Matrix plot of Tweet Level Entity features across Bot and Human classes

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} \PY{c+c1}{\PYZsh{} Create a Matrix plot across Bot and Human classes}
         \PY{n}{ax} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{p}{,} \PY{n}{hue}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{bot\PYZus{}flg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{palette}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{GnBu\PYZus{}d}\PY{l+s+s2}{\PYZdq{}}
                            \PY{p}{,} \PY{n+nb}{vars}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{media\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{url\PYZus{}ref\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{symbols\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}ref\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_32_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Heat map of the correlation matrix for the numeric features

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{c+c1}{\PYZsh{} Initialize the Figure}
         \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Calculate the correlation between the numeric features}
         \PY{n}{corr} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{user\PYZus{}lang}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{protected}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{verified}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}translator}\PY{l+s+s1}{\PYZsq{}}
                                            \PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{possibly\PYZus{}sensitive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{place\PYZus{}reference}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
                                          \PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{errors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Generate a heatmap on the correlation matrix}
         \PY{n}{ax} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{corr}\PY{p}{,}\PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Blues}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{linecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}ecf0f1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_34_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Distribution plot of Tweet Level Entities across Bot and Human classes

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}97}]:} \PY{c+c1}{\PYZsh{} Initialize the Figure with 2X2 subplots}
         \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{)}\PY{p}{)}
         \PY{n}{idx} \PY{o}{=} \PY{l+m+mi}{1}
         
         \PY{c+c1}{\PYZsh{} for all entity counts, plot a overlaying histogram subplot of Bot and Human classes}
         \PY{k}{for} \PY{n}{cnt} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{media\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{url\PYZus{}ref\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}ref\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{idx}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{df\PYZus{}bot}\PY{p}{[}\PY{n}{cnt}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}2c3e50}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{density}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{df\PYZus{}human}\PY{p}{[}\PY{n}{cnt}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}e74c3c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Human}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{density}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{cnt}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{16}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Distribution of \PYZhy{} }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n}{cnt}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{16}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{12}\PY{p}{)}
             \PY{n}{idx} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
             
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_36_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Distribution plot of User Level Count features across Bot and Human
classes

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}98}]:} \PY{c+c1}{\PYZsh{} Initialize the Figure with 2X2 subplots}
         \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{)}\PY{p}{)}
         \PY{n}{idx} \PY{o}{=} \PY{l+m+mi}{1}
         
         \PY{c+c1}{\PYZsh{} for all user level counts, plot a overlaying histogram subplot of Bot and Human classes}
         \PY{k}{for} \PY{n}{cnt} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{favourites\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{followers\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{friends\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{statuses\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{idx}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{df\PYZus{}bot}\PY{p}{[}\PY{n}{cnt}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}16a085}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{density}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{df\PYZus{}human}\PY{p}{[}\PY{n}{cnt}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}d35400}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Human}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{density}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{cnt}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{16}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Distribution of \PYZhy{} }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n}{cnt}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{16}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{12}\PY{p}{)}
             \PY{n}{idx} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
             
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_38_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Distribution plot of Tweet Frequency in Hours across Bot and Human
classes

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}99}]:} \PY{c+c1}{\PYZsh{} Initialize the Figure with 2X2 subplots}
         \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{idx} \PY{o}{=} \PY{l+m+mi}{1}
         
         \PY{c+c1}{\PYZsh{} for tweeting frequency, plot a overlaying histogram subplot of Bot and Human classes}
         \PY{k}{for} \PY{n}{cnt} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tweet\PYZus{}freq\PYZus{}hrs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{idx}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{df\PYZus{}bot}\PY{p}{[}\PY{n}{cnt}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}8e44ad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{density}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{df\PYZus{}human}\PY{p}{[}\PY{n}{cnt}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}c0392b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Human}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{density}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{cnt}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{16}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Distribution of Tweet Frequency in Hours }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{16}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{12}\PY{p}{)}
             \PY{n}{idx} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
             
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_40_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Distribution plot of Sentiment analysis scores of Tweets across Bot and
Human classes

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}100}]:} \PY{c+c1}{\PYZsh{} Initialize the Figure with 2X2 subplots}
          \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
          \PY{n}{idx} \PY{o}{=} \PY{l+m+mi}{1}
          
          \PY{c+c1}{\PYZsh{} for sentiment analysis scores, plot a overlaying histogram subplot of Bot and Human classes}
          \PY{k}{for} \PY{n}{cnt} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{senti\PYZus{}subjectivity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{senti\PYZus{}polarity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{:}
              \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{idx}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{df\PYZus{}bot}\PY{p}{[}\PY{n}{cnt}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}3498db}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{density}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{df\PYZus{}human}\PY{p}{[}\PY{n}{cnt}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}2c3e50}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Human}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{density}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{cnt}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{16}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Distribution of \PYZhy{} }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n}{cnt}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{16}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{12}\PY{p}{)}
              \PY{n}{idx} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
              
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_42_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Distribution plot of Categorical features

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}101}]:} \PY{c+c1}{\PYZsh{} Initialize the Figure with 2X2 subplots}
          \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)}
          \PY{n}{idx} \PY{o}{=} \PY{l+m+mi}{1}
          
          \PY{c+c1}{\PYZsh{} for all categorical, plot a overlaying histogram subplot of Bot and Human classes}
          \PY{k}{for} \PY{n}{cnt} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{default\PYZus{}profile}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{default\PYZus{}profile\PYZus{}image}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{geo\PYZus{}enabled}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{protected}\PY{l+s+s1}{\PYZsq{}}
                      \PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{verified}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{has\PYZus{}extended\PYZus{}profile}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}translator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{place\PYZus{}reference}\PY{l+s+s1}{\PYZsq{}}
                      \PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{possibly\PYZus{}sensitive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}a\PYZus{}reply}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}quote\PYZus{}status}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}a\PYZus{}retweet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{:}
              \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{idx}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{df\PYZus{}bot}\PY{p}{[}\PY{n}{cnt}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}9b59b6}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{df\PYZus{}human}\PY{p}{[}\PY{n}{cnt}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}16a085}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Human}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{cnt}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{12}\PY{p}{)}
              \PY{n}{idx} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
              
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_44_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Distribution of User Languages across User classes

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}102}]:} \PY{c+c1}{\PYZsh{} Initialize the Figure}
          \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} for Both human and bot classes, plot a bar plot subplot of different languages used}
          \PY{n}{ax} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{user\PYZus{}lang}\PY{p}{,} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{bot\PYZus{}flg}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_46_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Feature Importance

\textbf{Drop features that does not seem to differing distribution
patterns between the human and bot classes in the EDA Process and thene
evaluate the importance of the remaining features using Extra Trees
Classifier}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}103}]:} \PY{c+c1}{\PYZsh{} Drop the categorical values that does not seem to be providing value based on EDA}
          \PY{n}{processed\PYZus{}data\PYZus{}df} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{protected}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{verified}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}translator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{default\PYZus{}profile\PYZus{}image}\PY{l+s+s1}{\PYZsq{}}
                                                            \PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{possibly\PYZus{}sensitive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{place\PYZus{}reference}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{user\PYZus{}lang}\PY{l+s+s2}{\PYZdq{}}
                                                         \PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{default\PYZus{}profile}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{symbols\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}
                                                        \PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{errors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          
          \PY{c+c1}{\PYZsh{} Instantiate an ExtraTreeClassifier model with 100 estimators and fit with the predictor data set}
          \PY{n}{ext\PYZus{}tree\PYZus{}clf} \PY{o}{=} \PY{n}{ExtraTreesClassifier}\PY{p}{(}\PY{n}{n\PYZus{}estimators} \PY{o}{=} \PY{l+m+mi}{100}\PY{p}{)}
          \PY{n}{ext\PYZus{}tree\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{processed\PYZus{}data\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{bot\PYZus{}flg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{processed\PYZus{}data\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{bot\PYZus{}flg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Evaluate the feature importance from the fit model}
          \PY{n}{feature\PYZus{}importance} \PY{o}{=} \PY{n}{ext\PYZus{}tree\PYZus{}clf}\PY{o}{.}\PY{n}{feature\PYZus{}importances\PYZus{}}
          
          \PY{c+c1}{\PYZsh{} Calculate relative importance score of the features and plot them in descending order of importance (High to Low)}
          \PY{n}{feature\PYZus{}importance} \PY{o}{=} \PY{l+m+mf}{100.0} \PY{o}{*} \PY{p}{(}\PY{n}{feature\PYZus{}importance} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{feature\PYZus{}importance}\PY{p}{)}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{)}
          \PY{n}{sorted\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argsort}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{feature\PYZus{}importance}\PY{p}{)}\PY{p}{)}
          \PY{n}{pos} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{sorted\PYZus{}idx}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{o}{.}\PY{l+m+mi}{5}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{barh}\PY{p}{(}\PY{n}{pos}\PY{p}{,} \PY{n}{feature\PYZus{}importance}\PY{p}{[}\PY{n}{sorted\PYZus{}idx}\PY{p}{]}\PY{p}{,} \PY{n}{align}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{center}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{n}{pos}\PY{p}{,} \PY{n}{processed\PYZus{}data\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{bot\PYZus{}flg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{n}{sorted\PYZus{}idx}\PY{p}{]}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Relative Importance score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature Importance prediction based on Extra Trees Classifier}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{tick\PYZus{}params}\PY{p}{(}\PY{n}{labelsize}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_48_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Topic Based Modeling - Distribution of Hash tag topics used

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}104}]:} \PY{c+c1}{\PYZsh{} Plot the most used topics from the given set of topics}
          \PY{k}{def} \PY{n+nf}{plot\PYZus{}most\PYZus{}used\PYZus{}topics}\PY{p}{(}\PY{n}{topics}\PY{p}{,} \PY{n}{class\PYZus{}type}\PY{p}{,} \PY{n}{top\PYZus{}n}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Calculate the Most used topics from the given set of topic and }
          \PY{l+s+sd}{        plot the top\PYZus{}n topics and the number of times they have been used}
          \PY{l+s+sd}{       }
          \PY{l+s+sd}{       Args:}
          \PY{l+s+sd}{           topics: Dataseries containing all the topics to assess.}
          \PY{l+s+sd}{           class\PYZus{}type: Type of the user class being assessed}
          \PY{l+s+sd}{           top\PYZus{}n: count of top n features to plot. Default is 20}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
              
              \PY{c+c1}{\PYZsh{} Extract the consolidated topics in a list as a separte topic items}
              \PY{n}{text\PYZus{}clean} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{p}{[}\PY{n}{text} \PY{k}{for} \PY{n}{text} \PY{o+ow}{in} \PY{n}{topics}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}
              \PY{n}{text\PYZus{}clean} \PY{o}{=} \PY{p}{[} \PY{n}{x} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{text\PYZus{}clean} \PY{k}{if} \PY{n}{x} \PY{o+ow}{is} \PY{o+ow}{not} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
              
              \PY{c+c1}{\PYZsh{} Use counter to count the number of times each topic is used and extract the top n used for plotting}
              \PY{n}{counts} \PY{o}{=} \PY{n}{Counter}\PY{p}{(}\PY{n}{text\PYZus{}clean}\PY{p}{)}
              \PY{n}{common\PYZus{}words} \PY{o}{=} \PY{p}{[}\PY{n}{word}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{counts}\PY{o}{.}\PY{n}{most\PYZus{}common}\PY{p}{(}\PY{n}{top\PYZus{}n}\PY{p}{)}\PY{p}{]}
              \PY{n}{common\PYZus{}counts} \PY{o}{=} \PY{p}{[}\PY{n}{word}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{counts}\PY{o}{.}\PY{n}{most\PYZus{}common}\PY{p}{(}\PY{n}{top\PYZus{}n}\PY{p}{)}\PY{p}{]}
          
              \PY{c+c1}{\PYZsh{} Plot a Bar plot for the top n topics}
              \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
              \PY{n}{sns}\PY{o}{.}\PY{n}{barplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{common\PYZus{}words}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{common\PYZus{}counts}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{top\PYZus{}n}\PY{p}{)} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Most Common topics discussed in }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{n}{class\PYZus{}type} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ tweets}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{90}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{tick\PYZus{}params}\PY{p}{(}\PY{n}{labelsize}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}105}]:} \PY{c+c1}{\PYZsh{} Plot the top 30 Topics discussed by Bot class users from our training data set}
          \PY{n}{plot\PYZus{}most\PYZus{}used\PYZus{}topics}\PY{p}{(}\PY{n}{df\PYZus{}bot}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Bot}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Plot the top 30 Topics discussed by Human class users from our training data set}
          \PY{n}{plot\PYZus{}most\PYZus{}used\PYZus{}topics}\PY{p}{(}\PY{n}{df\PYZus{}human}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Human}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_51_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_51_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Cummulative plot of PCA variance ratio of principal features

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}106}]:} \PY{c+c1}{\PYZsh{} Initialize PCA for 19 components and fit on the normalized training data}
          \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{19}\PY{p}{)}
          \PY{n}{pca}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{processed\PYZus{}data\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{bot\PYZus{}flg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Initialize a figure for the Variance plot}
          \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Draw a scatter plot on the cumulative sum on the explained variance percentage for the principal components}
          \PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{cumsum}\PY{p}{(}\PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{p}{)}
                     \PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Variance Ratio}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Define Plot Labels and legends}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PCA Dimension}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{16}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cumulative Variance}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{16}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{step}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{14}\PY{p}{)}\PY{p}{;}
          
          \PY{c+c1}{\PYZsh{} Determine the number of principal components at which 95\PYZpc{} variance is explained}
          \PY{n}{comp\PYZus{}count} \PY{o}{=} \PY{l+m+mi}{0}
          \PY{n}{cum\PYZus{}sum} \PY{o}{=} \PY{l+m+mi}{0}
          \PY{k}{for} \PY{n}{ratio} \PY{o+ow}{in} \PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{p}{:}
              \PY{n}{comp\PYZus{}count} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
              \PY{n}{cum\PYZus{}sum} \PY{o}{+}\PY{o}{=} \PY{n}{ratio}
              \PY{k}{if} \PY{n}{cum\PYZus{}sum} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mf}{0.95} \PY{p}{:}
                  \PY{k}{break}
          
          \PY{c+c1}{\PYZsh{} Draw a vertical axis line to indicate the optimal number of principal components}
          \PY{n}{plt}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{n}{comp\PYZus{}count}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of principal components explaining atleast 95}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{ variance \PYZhy{} }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{comp\PYZus{}count}\PY{p}{)}
                  
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Number of principal components explaining atleast 95\% variance -  4

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_53_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Dataset splitting, One-hot encoding of categorical features, Scaling and
Normalization

\textbf{Scale and Normalize data attributes to follow an uniform scaling
for proper weightage of features by the model}\\
\textbf{Perform One-hot encoding for category based features}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}107}]:} \PY{c+c1}{\PYZsh{} Perform One Hot Encoding to add dummy columns for categorical values of User Language feature}
          \PY{c+c1}{\PYZsh{} processed\PYZus{}data\PYZus{}df=pd.get\PYZus{}dummies(processed\PYZus{}data\PYZus{}df, columns=[\PYZsq{}user\PYZus{}lang\PYZsq{}], drop\PYZus{}first=True)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}108}]:} \PY{c+c1}{\PYZsh{} Method Implementation to Separate the X (Predictor) and y (Response) variables in a data set}
          \PY{k}{def} \PY{n+nf}{separate\PYZus{}X\PYZus{}and\PYZus{}y}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{outcome\PYZus{}col\PYZus{}name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Separate the predictor variables and outcome variable to separate dataframes}
          \PY{l+s+sd}{       }
          \PY{l+s+sd}{       Args:}
          \PY{l+s+sd}{           data: Dataframe containing predictor and Response variables.}
          \PY{l+s+sd}{           outcome\PYZus{}col\PYZus{}name: Name of the response column to extract}
          \PY{l+s+sd}{       Returns:}
          \PY{l+s+sd}{           X\PYZus{}df: Normalized predictor features of Training data set.}
          \PY{l+s+sd}{           y\PYZus{}df: Outcome variable list of Training data set.}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
              \PY{c+c1}{\PYZsh{} Separate Predictor and Response variables in dataset}
              \PY{n}{X\PYZus{}df} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{n}{outcome\PYZus{}col\PYZus{}name}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
              \PY{n}{y\PYZus{}df} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{outcome\PYZus{}col\PYZus{}name}\PY{p}{]}
              
              \PY{c+c1}{\PYZsh{} Reset Index of data set for consistency}
              \PY{n}{X\PYZus{}df} \PY{o}{=} \PY{n}{X\PYZus{}df}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{n}{drop}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
              \PY{n}{y\PYZus{}df} \PY{o}{=} \PY{n}{y\PYZus{}df}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{n}{drop}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
              
              \PY{k}{return} \PY{n}{X\PYZus{}df}\PY{p}{,} \PY{n}{y\PYZus{}df}
          
          \PY{c+c1}{\PYZsh{} Method Implementation to split data to train and test data set and scale predictor features}
          \PY{k}{def} \PY{n+nf}{split\PYZus{}train\PYZus{}and\PYZus{}test}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{outcome\PYZus{}col\PYZus{}name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{scale\PYZus{}type}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MinMax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{split\PYZus{}ratio}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Split the data into train and test data set and scale them}
          \PY{l+s+sd}{       }
          \PY{l+s+sd}{       Args:}
          \PY{l+s+sd}{           data: Dataframe containing predictor and Response variables.}
          \PY{l+s+sd}{           outcome\PYZus{}col\PYZus{}name: Name of the response column to extract}
          \PY{l+s+sd}{           Scale\PYZus{}type: Type of scaling to perform \PYZsq{}MinMax\PYZsq{} or \PYZsq{}Standard\PYZsq{}}
          \PY{l+s+sd}{           split\PYZus{}ratio: Ratio of data to split in training and test data set}
          \PY{l+s+sd}{       Returns:}
          \PY{l+s+sd}{           X\PYZus{}train\PYZus{}scale: Scaled predictor features of Training data set.}
          \PY{l+s+sd}{           y\PYZus{}train: Outcome variable list of Training data set.}
          \PY{l+s+sd}{           X\PYZus{}val\PYZus{}scale: Scaled predictor features of Test data set.}
          \PY{l+s+sd}{           y\PYZus{}val: Outcome variable list of Test data set.}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
              
              \PY{c+c1}{\PYZsh{} Make a 80\PYZhy{}20 Split of the data to get training and test data}
              \PY{n}{data\PYZus{}train}\PY{p}{,} \PY{n}{data\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{90}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{n}{split\PYZus{}ratio}
                                                       \PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{data}\PY{p}{[}\PY{n}{outcome\PYZus{}col\PYZus{}name}\PY{p}{]}\PY{p}{)} 
                      
              \PY{c+c1}{\PYZsh{} Separate Predictor and Response variables in Training and Test dataset}
              \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{separate\PYZus{}X\PYZus{}and\PYZus{}y}\PY{p}{(}\PY{n}{data\PYZus{}train}\PY{p}{,} \PY{n}{outcome\PYZus{}col\PYZus{}name}\PY{p}{)}
              \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{separate\PYZus{}X\PYZus{}and\PYZus{}y}\PY{p}{(}\PY{n}{data\PYZus{}test}\PY{p}{,} \PY{n}{outcome\PYZus{}col\PYZus{}name}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} Check scale\PYZus{}type and use appropriate scaler}
              \PY{k}{if} \PY{n}{scale\PYZus{}type} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MinMax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                  \PY{c+c1}{\PYZsh{} Initialize a Min Max scaler to normalize all predcitor variables between 0 and 1}
                  \PY{n}{scaler} \PY{o}{=} \PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}
              \PY{k}{else}\PY{p}{:}
                  \PY{c+c1}{\PYZsh{} Initialize a Standard scaler to scale all predcitor variables based on mean and std}
                  \PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} Fit scaler based on Training data}
              \PY{n}{scaler}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} Transform to Normalize Training data set}
              \PY{n}{X\PYZus{}train\PYZus{}scaled} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{p}{)}
              \PY{n}{X\PYZus{}train\PYZus{}scale} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scaled}\PY{p}{)}
              \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} Transform to Normalize Test data set}
              \PY{n}{X\PYZus{}val\PYZus{}scaled} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{values}\PY{p}{)}
              \PY{n}{X\PYZus{}val\PYZus{}scale} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}val\PYZus{}scaled}\PY{p}{)}
              \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
                  
              \PY{k}{return} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}val}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}109}]:} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{split\PYZus{}train\PYZus{}and\PYZus{}test}\PY{p}{(}\PY{n}{processed\PYZus{}data\PYZus{}df}\PY{p}{)}
\end{Verbatim}


    \section{ Model Training and
Validation}\label{model-training-and-validation}

\textbf{Creation, Training and Validation of classification models}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}110}]:} \PY{c+c1}{\PYZsh{} Dictionary of Models to compare}
          \PY{n}{models\PYZus{}to\PYZus{}compare} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
          \PY{n}{acc\PYZus{}stats\PYZus{}table\PYZus{}data} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
          
          \PY{n}{TRAIN} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train}\PY{l+s+s1}{\PYZsq{}}
          \PY{n}{VALIDATION} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation}\PY{l+s+s1}{\PYZsq{}}
          \PY{n}{TEST} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}


    \textbf{Utility Method for gathering model accuracy and model
comparisons}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}184}]:} \PY{c+c1}{\PYZsh{} Method implementation to calculate classifier prediction accuracy and CV scores}
          \PY{k}{def} \PY{n+nf}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{X\PYZus{}data}\PY{p}{,} \PY{n}{y\PYZus{}data}\PY{p}{,} \PY{n}{model\PYZus{}title}\PY{p}{,} \PY{n}{data\PYZus{}type}\PY{o}{=}\PY{n}{TRAIN}\PY{p}{,} \PY{n}{cv\PYZus{}fold}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{print\PYZus{}stats}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Calculate the Accuracy and Mean Cross validation score of the model}
          \PY{l+s+sd}{        Add the model to the comparison Dictionary}
          \PY{l+s+sd}{        }
          \PY{l+s+sd}{       Args:}
          \PY{l+s+sd}{           model The classifier model used to fit and predict the classifiers}
          \PY{l+s+sd}{           X\PYZus{}data: Dataframe containing the predictor variables.}
          \PY{l+s+sd}{           y\PYZus{}data: Dataframe containing the outcome variable.}
          \PY{l+s+sd}{           model\PYZus{}title: Title of the model to use for reporting}
          \PY{l+s+sd}{           data\PYZus{}type: Type of data being plotted (Train/Validation/Test). Default value is Train}
          \PY{l+s+sd}{           cv\PYZus{}fold: Cross Validation Fold used. Default value is 5}
          \PY{l+s+sd}{           print\PYZus{}stats: Flag to indicate if accuracy stats must be printed. Default is True}
          \PY{l+s+sd}{        Return:}
          \PY{l+s+sd}{           accuracy: Prediction accuracy Score of the model}
          \PY{l+s+sd}{           cv\PYZus{}scores: Cross validation scores of the model}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
              
              \PY{c+c1}{\PYZsh{} Add to the model list to compare}
              \PY{n}{models\PYZus{}to\PYZus{}compare}\PY{p}{[}\PY{n}{model\PYZus{}title}\PY{p}{]} \PY{o}{=} \PY{n}{model}
          
              \PY{c+c1}{\PYZsh{} Generate predictions for the given data set}
              \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}data}\PY{p}{)}\PY{p}{)}
              
              \PY{n}{accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}data}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
              \PY{c+c1}{\PYZsh{} Report stats only when print flag is set to True}
              \PY{k}{if} \PY{n}{print\PYZus{}stats} \PY{o}{==} \PY{k+kc}{True}\PY{p}{:}
                  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{ Model \PYZhy{} (}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{) Prediction Accuracy: }\PY{l+s+si}{\PYZpc{}0.2f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{model\PYZus{}title}\PY{p}{,} \PY{n}{data\PYZus{}type}\PY{p}{,} \PY{n}{accuracy}\PY{p}{)}\PY{p}{)}
              
              \PY{n}{cv\PYZus{}mean} \PY{o}{=} \PY{l+m+mi}{0}
              \PY{n}{cv\PYZus{}std} \PY{o}{=} \PY{l+m+mi}{0}
              \PY{n}{cv\PYZus{}scores} \PY{o}{=} \PY{k+kc}{None}
              \PY{c+c1}{\PYZsh{} Check if the model has a score method before CV validation}
              \PY{k}{if} \PY{n+nb}{hasattr}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o+ow}{and} \PY{n}{data\PYZus{}type}\PY{o}{==}\PY{n}{TRAIN}\PY{p}{:}
                  \PY{c+c1}{\PYZsh{} Calculate Cross validation accuracy score of the model using x fold cross validation}
                  \PY{n}{cv\PYZus{}scores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{X\PYZus{}data}\PY{p}{,} \PY{n}{y\PYZus{}data}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{cv\PYZus{}fold}\PY{p}{)}
                  \PY{n}{cv\PYZus{}mean} \PY{o}{=} \PY{n}{cv\PYZus{}scores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                  \PY{n}{cv\PYZus{}std} \PY{o}{=} \PY{n}{cv\PYZus{}scores}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
                  
                  \PY{k}{if} \PY{n}{print\PYZus{}stats} \PY{o}{==} \PY{k+kc}{True}\PY{p}{:}
                      \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{ (}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{) Cross Validation accuracy and 95 percent CI: }\PY{l+s+si}{\PYZpc{}0.2f}\PY{l+s+s2}{ (+/\PYZhy{} }\PY{l+s+si}{\PYZpc{}0.2f}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}} 
                                \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{model\PYZus{}title}\PY{p}{,} \PY{n}{data\PYZus{}type}\PY{p}{,} \PY{n}{cv\PYZus{}mean}\PY{p}{,} \PY{n}{cv\PYZus{}std} \PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
          
                  \PY{c+c1}{\PYZsh{} Plot the prediction accuracy and cross validation scores for cv\PYZus{}fold iterations}
                  \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{cv\PYZus{}fold}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{cv\PYZus{}scores}\PY{p}{,} \PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}c0392b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CV Scores}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{cv\PYZus{}fold}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{p}{[}\PY{n}{cv\PYZus{}mean} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*} \PY{n}{cv\PYZus{}std}\PY{p}{)}\PY{p}{]} \PY{o}{*} \PY{n}{cv\PYZus{}fold} \PY{p}{,}\PY{p}{[}\PY{n}{cv\PYZus{}mean} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*} \PY{n}{cv\PYZus{}std}\PY{p}{)}\PY{p}{]} \PY{o}{*} \PY{n}{cv\PYZus{}fold}
                                   \PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}1abc9c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{+/\PYZhy{} 2 SDev of CV mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{axhline}\PY{p}{(}\PY{n}{accuracy}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}2c3e50}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
          
                  \PY{c+c1}{\PYZsh{} Set title, axis values and legends for the plot}
                  \PY{n}{bot} \PY{o}{=} \PY{l+m+mf}{0.7}
                  \PY{k}{if} \PY{n}{cv\PYZus{}scores}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{l+m+mf}{0.75}\PY{p}{:}
                      \PY{n}{bot} \PY{o}{=} \PY{n}{cv\PYZus{}scores}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.05}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{model\PYZus{}title} \PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ \PYZhy{} Prediction accuracy and CV scores for each fold \PYZhy{} }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n}{data\PYZus{}type}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CV fold iterations}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{n}{bottom}\PY{o}{=}\PY{n}{bot}\PY{p}{,} \PY{n}{top}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          
                  \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{n}{left}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{right}\PY{o}{=}\PY{n}{cv\PYZus{}fold}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
              \PY{k}{else}\PY{p}{:}
                  \PY{n}{cv\PYZus{}mean} \PY{o}{=} \PY{n}{accuracy}
          
                  
              \PY{c+c1}{\PYZsh{} Add accuracy stats to the stats table}
              \PY{n}{type\PYZus{}stats} \PY{o}{=} \PY{n}{acc\PYZus{}stats\PYZus{}table\PYZus{}data}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{data\PYZus{}type}\PY{p}{,} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{;}
              \PY{k}{if} \PY{n}{model\PYZus{}title} \PY{o}{!=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Topic Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                  \PY{k}{if} \PY{n}{data\PYZus{}type}\PY{o}{==}\PY{n}{TRAIN}\PY{p}{:}
                      \PY{n}{type\PYZus{}stats}\PY{p}{[}\PY{n}{model\PYZus{}title}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{model\PYZus{}title}\PY{p}{,} \PY{n+nb}{round}\PY{p}{(}\PY{n}{accuracy}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n+nb}{round}\PY{p}{(}\PY{n}{cv\PYZus{}mean}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
                                                 \PY{p}{,} \PY{n+nb}{round}\PY{p}{(}\PY{n}{cv\PYZus{}std}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}data}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{]}
                  \PY{k}{else}\PY{p}{:}
                      \PY{n}{type\PYZus{}stats}\PY{p}{[}\PY{n}{model\PYZus{}title}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{model\PYZus{}title}\PY{p}{,} \PY{n+nb}{round}\PY{p}{(}\PY{n}{accuracy}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}data}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{]}
              
                  \PY{n}{acc\PYZus{}stats\PYZus{}table\PYZus{}data}\PY{p}{[}\PY{n}{data\PYZus{}type}\PY{p}{]} \PY{o}{=} \PY{n}{type\PYZus{}stats}\PY{p}{;}
                  
              \PY{k}{return} \PY{n}{accuracy}\PY{p}{,} \PY{n}{cv\PYZus{}scores}
          
          \PY{c+c1}{\PYZsh{} Render the Accuracy stats of Model in tabular format}
          \PY{k}{def} \PY{n+nf}{renderAccuracyStatsIable}\PY{p}{(}\PY{n}{stats\PYZus{}table\PYZus{}data}\PY{p}{,} \PY{n}{data\PYZus{}type}\PY{p}{,} \PY{n}{plot\PYZus{}type}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CV Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Method to Render the Accuracy stats of Model in tabular format}
          \PY{l+s+sd}{        }
          \PY{l+s+sd}{       Args:}
          \PY{l+s+sd}{           stats\PYZus{}table\PYZus{}data: List of Lists containing model accuracy stats in a tabular format}
          \PY{l+s+sd}{           data\PYZus{}type: Type of the data for which the status has been gathered for (TRAIN/VALIDATION/TEST)}
          \PY{l+s+sd}{           plot\PYZus{}type: Type of score to be used to render the bar plots. Default is \PYZsq{}CV Accuracy\PYZsq{}}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
              \PY{n}{type\PYZus{}stats} \PY{o}{=} \PY{n}{stats\PYZus{}table\PYZus{}data}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{data\PYZus{}type}\PY{p}{,} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{;}
              
              \PY{c+c1}{\PYZsh{} Title of Column Labels to report Accuracy}
              \PY{k}{if} \PY{n}{data\PYZus{}type}\PY{o}{==}\PY{n}{TRAIN}\PY{p}{:}
                  \PY{n}{colLabels} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CV Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{95}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{ CI (+/\PYZhy{})}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TN, FP, FN, TP}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
              \PY{k}{else}\PY{p}{:}
                  \PY{n}{colLabels} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TN, FP, FN, TP}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
                  \PY{n}{plot\PYZus{}type}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction Accuracy}\PY{l+s+s1}{\PYZsq{}}
          
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{** }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ ** Comparison Table of Accuracy stats for different classifiation models}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{data\PYZus{}type}\PY{p}{)}\PY{p}{)}
              \PY{c+c1}{\PYZsh{} Render the Stats of different models in a table for better comparability}
              \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
              \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{,} \PY{n}{frame\PYZus{}on}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)} 
              
              \PY{c+c1}{\PYZsh{} Do some data prep for table}
              \PY{n}{models} \PY{o}{=} \PY{p}{[}\PY{p}{]}
              \PY{n}{cv\PYZus{}scrs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
              \PY{n}{acc\PYZus{}scrs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
              \PY{n}{table\PYZus{}data} \PY{o}{=} \PY{p}{[}\PY{p}{]}
              \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{val} \PY{o+ow}{in} \PY{n}{type\PYZus{}stats}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                  \PY{n}{models}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{val}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                  \PY{n}{acc\PYZus{}scrs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{val}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
                  \PY{n}{cv\PYZus{}scrs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{val}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
                  \PY{n}{table\PYZus{}data}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{val}\PY{p}{)}
          
              \PY{n}{tbl} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{table}\PY{p}{(}\PY{n}{cellText}\PY{o}{=}\PY{n}{table\PYZus{}data}\PY{p}{,} \PY{n}{colLabels}\PY{o}{=}\PY{n}{colLabels}\PY{p}{,}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{top}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{tbl}\PY{o}{.}\PY{n}{set\PYZus{}fontsize}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{)}
              \PY{n}{tbl}\PY{o}{.}\PY{n}{scale}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} Set colors for the bar plots}
              \PY{n}{bar\PYZus{}color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}16a085}\PY{l+s+s1}{\PYZsq{}}
              \PY{n}{high\PYZus{}color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}e67e22}\PY{l+s+s1}{\PYZsq{}}
              \PY{k}{if} \PY{n}{data\PYZus{}type} \PY{o}{==} \PY{n}{TEST}\PY{p}{:}
                  \PY{n}{bar\PYZus{}color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}2980b9}\PY{l+s+s1}{\PYZsq{}}
                  \PY{n}{high\PYZus{}color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}34495e}\PY{l+s+s1}{\PYZsq{}}
              
              \PY{n}{ylabel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cross Validation Score}\PY{l+s+s1}{\PYZsq{}}   
              \PY{n}{bar\PYZus{}scrs} \PY{o}{=} \PY{n}{cv\PYZus{}scrs}
              \PY{k}{if} \PY{n}{plot\PYZus{}type} \PY{o}{!=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CV Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                  \PY{n}{ylabel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction Accuracy Score}\PY{l+s+s1}{\PYZsq{}}   
                  \PY{n}{bar\PYZus{}scrs} \PY{o}{=} \PY{n}{acc\PYZus{}scrs}
              
              \PY{c+c1}{\PYZsh{} Render a bar chart of the model accuracy scores}
              \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{n}{bottom}\PY{o}{=}\PY{n+nb}{min}\PY{p}{(}\PY{n}{bar\PYZus{}scrs}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{top}\PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n+nb}{max}\PY{p}{(}\PY{n}{bar\PYZus{}scrs}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
              \PY{n}{y\PYZus{}pos} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{models}\PY{p}{)}\PY{p}{)}
              \PY{n}{barlist} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{y\PYZus{}pos}\PY{p}{,} \PY{n}{bar\PYZus{}scrs}\PY{p}{,} \PY{n}{align}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{center}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n}{bar\PYZus{}color}\PY{p}{)}
              \PY{n}{barlist}\PY{p}{[}\PY{n}{bar\PYZus{}scrs}\PY{o}{.}\PY{n}{index}\PY{p}{(}\PY{n+nb}{max}\PY{p}{(}\PY{n}{bar\PYZus{}scrs}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}color}\PY{p}{(}\PY{n}{high\PYZus{}color}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{y\PYZus{}pos}\PY{p}{,} \PY{n}{models}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{n}{ylabel}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Classification Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              
              \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    Trivial Classification - Baseline Model

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}112}]:} \PY{c+c1}{\PYZsh{} Implement a trivial model that predicts Human class (0) for any input}
          \PY{k}{def} \PY{n+nf}{predict\PYZus{}trivial}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:} 
              \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Make trivial predictions on the train data set and calculate scores}
          \PY{n}{trivial\PYZus{}predictions} \PY{o}{=} \PY{n}{predict\PYZus{}trivial}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{)}
          \PY{n}{baseline\PYZus{}accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{trivial\PYZus{}predictions}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Baseline Accuracy from Trivial Model:}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{n}{baseline\PYZus{}accuracy}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Baseline Accuracy from Trivial Model:0.67

    \end{Verbatim}

    Logistic Regression CV

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}113}]:} \PY{n}{LOGISTIC\PYZus{}CLF} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Logistic CV}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Model Title constant for Logistic Classifier}
          
          \PY{c+c1}{\PYZsh{} Initialize a Logistic CV Regression model with L2 regualarization}
          \PY{n}{lcv\PYZus{}clf} \PY{o}{=} \PY{n}{LogisticRegressionCV}\PY{p}{(}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{penalty}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{multi\PYZus{}class} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ovr}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Fit the model with the training set data}
          \PY{n}{lcv\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Report Prediction accuracy on Train and Validation Data sets}
          \PY{n}{lcv\PYZus{}train\PYZus{}acc}\PY{p}{,} \PY{n}{lcv\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{lcv\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{LOGISTIC\PYZus{}CLF} \PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
          \PY{n}{lcv\PYZus{}val\PYZus{}acc}\PY{p}{,} \PY{n}{lcv\PYZus{}val\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{lcv\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{LOGISTIC\PYZus{}CLF}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Logistic CV Model - (Train) Prediction Accuracy: 0.81
Logistic CV (Train) Cross Validation accuracy and 95 percent CI: 0.81 (+/- 0.05)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Logistic CV Model - (Validation) Prediction Accuracy: 0.83

    \end{Verbatim}

    Principal Component Analysis (PCA) Classifier

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}114}]:} \PY{n}{PCA\PYZus{}CLF} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PCA}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Model Title constant for PCA Classifier}
          
          \PY{c+c1}{\PYZsh{} Initialize a PCA instnace for 4 principal components and fit using the scaled training data}
          \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{comp\PYZus{}count}\PY{p}{)}
          \PY{n}{pca}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Transform the scaled training and test data set using the fitted PCA model}
          \PY{n}{X\PYZus{}train\PYZus{}pca\PYZus{}4} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{)}
          \PY{n}{X\PYZus{}val\PYZus{}pca\PYZus{}4} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Instantiate a Logisitic classification model for the data set transformed with 3 principal components}
          \PY{c+c1}{\PYZsh{}pca\PYZus{}3\PYZus{}clf = LogisticRegressionCV(cv=5, penalty=\PYZsq{}l2\PYZsq{}, max\PYZus{}iter=1000, multi\PYZus{}class = \PYZsq{}ovr\PYZsq{})}
          \PY{n}{pca\PYZus{}4\PYZus{}clf} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{C}\PY{o}{=}\PY{l+m+mi}{1000000}\PY{p}{,} \PY{n}{solver}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{newton\PYZhy{}cg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{250}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Fit the model with the training set data}
          \PY{n}{pca\PYZus{}4\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}pca\PYZus{}4}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Report Prediction accuracy on Train and Validation Data sets}
          \PY{n}{pca\PYZus{}4\PYZus{}train\PYZus{}acc}\PY{p}{,} \PY{n}{pca\PYZus{}4\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{pca\PYZus{}4\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}pca\PYZus{}4}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{PCA\PYZus{}CLF}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
          \PY{n}{pca\PYZus{}4\PYZus{}val\PYZus{}acc}\PY{p}{,} \PY{n}{pca\PYZus{}4\PYZus{}val\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{pca\PYZus{}4\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}pca\PYZus{}4}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{PCA\PYZus{}CLF}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
PCA Model - (Train) Prediction Accuracy: 0.80
PCA (Train) Cross Validation accuracy and 95 percent CI: 0.80 (+/- 0.06)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_67_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
PCA Model - (Validation) Prediction Accuracy: 0.82

    \end{Verbatim}

    Linear Discriminant Analysis (LDA) Classifier

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}115}]:} \PY{n}{LDA\PYZus{}CLF} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LDA}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Model Title constant for LDA Classifier}
          
          \PY{c+c1}{\PYZsh{} Initialize a LDA Model}
          \PY{n}{lda} \PY{o}{=} \PY{n}{LinearDiscriminantAnalysis}\PY{p}{(}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Fit the model with the training set data}
          \PY{n}{lda}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Report Prediction accuracy on Train and Validation Data sets}
          \PY{n}{lda\PYZus{}train\PYZus{}acc}\PY{p}{,} \PY{n}{lda\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{lda}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{LDA\PYZus{}CLF}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
          \PY{n}{lda\PYZus{}val\PYZus{}acc}\PY{p}{,} \PY{n}{lda\PYZus{}val\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{lda}\PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{LDA\PYZus{}CLF}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
LDA Model - (Train) Prediction Accuracy: 0.81
LDA (Train) Cross Validation accuracy and 95 percent CI: 0.80 (+/- 0.05)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_69_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
LDA Model - (Validation) Prediction Accuracy: 0.82

    \end{Verbatim}

    Quadratic Discriminant Analysis (QDA) Classifier

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}116}]:} \PY{n}{QDA\PYZus{}CLF} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{QDA}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Model Title constant for QDA Classifier}
          
          \PY{c+c1}{\PYZsh{} Initialize a LDA Model}
          \PY{n}{qda} \PY{o}{=} \PY{n}{QuadraticDiscriminantAnalysis}\PY{p}{(}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Fit the model with the training set data}
          \PY{n}{qda}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Report Prediction accuracy on Train and Validation Data sets}
          \PY{n}{qda\PYZus{}train\PYZus{}acc}\PY{p}{,} \PY{n}{qda\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{qda}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{QDA\PYZus{}CLF}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
          \PY{n}{qda\PYZus{}val\PYZus{}acc}\PY{p}{,} \PY{n}{qda\PYZus{}val\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{qda}\PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{QDA\PYZus{}CLF}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
QDA Model - (Train) Prediction Accuracy: 0.77
QDA (Train) Cross Validation accuracy and 95 percent CI: 0.76 (+/- 0.08)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_71_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
QDA Model - (Validation) Prediction Accuracy: 0.80

    \end{Verbatim}

    k-Nearest-Neighbors Classifier

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}117}]:} \PY{c+c1}{\PYZsh{} Instantiate lists to hold accuracy and fitted model per k\PYZhy{}value}
          \PY{n}{knn\PYZus{}accuracy} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{knn\PYZus{}cv\PYZus{}means} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{knn\PYZus{}models} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
          
          \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{=} \PY{l+m+mi}{0}
          \PY{n}{optimal\PYZus{}k} \PY{o}{=} \PY{l+m+mi}{0}
          \PY{n}{max\PYZus{}k} \PY{o}{=} \PY{l+m+mi}{51}
          \PY{c+c1}{\PYZsh{} Loop from k neighbors from 1 to max K}
          \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{max\PYZus{}k}\PY{p}{)}\PY{p}{:}
              \PY{c+c1}{\PYZsh{}Instantiate a kNN classifier for k neighbors}
              \PY{n}{knn\PYZus{}clf} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{n}{k}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} Fit for the training data    }
              \PY{n}{knn\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
                  
              \PY{c+c1}{\PYZsh{} Check accuracy using prediction on the training data}
              \PY{n}{predictions} \PY{o}{=} \PY{n}{knn\PYZus{}clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{)}
              \PY{n}{acc\PYZus{}score} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
              \PY{n}{knn\PYZus{}accuracy}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{acc\PYZus{}score}\PY{p}{)}
                 
              \PY{c+c1}{\PYZsh{} Check accuracy for 5\PYZhy{}fold cross validation test and take the mean}
              \PY{n}{cv\PYZus{}mean} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{knn\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
              \PY{n}{knn\PYZus{}cv\PYZus{}means}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cv\PYZus{}mean}\PY{p}{)}
              \PY{n}{knn\PYZus{}models}\PY{p}{[}\PY{n}{k}\PY{p}{]} \PY{o}{=} \PY{n}{knn\PYZus{}clf}
              
              \PY{c+c1}{\PYZsh{} update K with max CV Mean score}
              \PY{k}{if} \PY{n}{cv\PYZus{}mean} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}cv\PYZus{}mean}\PY{p}{:}
                  \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{=} \PY{n}{cv\PYZus{}mean}
                  \PY{n}{optimal\PYZus{}k} \PY{o}{=} \PY{n}{k}
          
          \PY{c+c1}{\PYZsh{} Plot the prediction accuracy andf 5 fold cross validation mean for each tree depth}
          \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{max\PYZus{}k}\PY{p}{)}\PY{p}{,} \PY{n}{knn\PYZus{}cv\PYZus{}means}\PY{p}{,}\PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}2c3e50}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{5 fold CV score Mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{max\PYZus{}k}\PY{p}{)}\PY{p}{,} \PY{n}{knn\PYZus{}accuracy}\PY{p}{,}\PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}c0392b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction Accuracy Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{n}{optimal\PYZus{}k}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Optimal K \PYZhy{} }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{optimal\PYZus{}k}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Set title, axis values and legends for the plot}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Plot of the prediction accuracy and CV mean score against each K value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{K neighbors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_73_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}118}]:} \PY{n}{KNN\PYZus{}CLF} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kNN, K\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{optimal\PYZus{}k}\PY{p}{)} \PY{c+c1}{\PYZsh{} Model Title constant for kNN Classifier}
          
          \PY{c+c1}{\PYZsh{} Report Prediction accuracy on Train and Validation Data sets}
          \PY{n}{knn\PYZus{}train\PYZus{}acc}\PY{p}{,} \PY{n}{knn\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{knn\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}k}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{KNN\PYZus{}CLF}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
          \PY{n}{knn\PYZus{}val\PYZus{}acc}\PY{p}{,} \PY{n}{knn\PYZus{}val\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{knn\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}k}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{KNN\PYZus{}CLF}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
kNN, K-13 Model - (Train) Prediction Accuracy: 0.82
kNN, K-13 (Train) Cross Validation accuracy and 95 percent CI: 0.80 (+/- 0.06)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_74_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
kNN, K-13 Model - (Validation) Prediction Accuracy: 0.82

    \end{Verbatim}

    Decision Tree Classifier

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}119}]:} \PY{c+c1}{\PYZsh{} Instantiate lists to hold tree depths, accuracy and fitted model per depth}
          \PY{n}{tree\PYZus{}depths}\PY{o}{=}\PY{p}{[}\PY{p}{]}
          \PY{n}{accuracy} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{cv\PYZus{}means} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{tree\PYZus{}models} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
          
          \PY{n}{optimal\PYZus{}depth} \PY{o}{=} \PY{l+m+mi}{0}
          \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{=} \PY{l+m+mi}{0}
          \PY{c+c1}{\PYZsh{} Loop from tree depth 2 to 10}
          \PY{k}{for} \PY{n}{depth} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
              \PY{c+c1}{\PYZsh{}Instantiate a Decision Tree classifier for the current depth using gini index as the criterion}
              \PY{n}{dt\PYZus{}clf} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{criterion}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gini}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{n}{depth}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} Fit for the training data    }
              \PY{n}{dt\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
                  
              \PY{c+c1}{\PYZsh{} Check accuracy using prediction on the training data}
              \PY{n}{predictions} \PY{o}{=} \PY{n}{dt\PYZus{}clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{)}
              \PY{n}{acc\PYZus{}score} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
              \PY{n}{accuracy}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{acc\PYZus{}score}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} Check accuracy for 5\PYZhy{}fold cross validation test and take the mean}
              \PY{n}{cv\PYZus{}mean} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{dt\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
              \PY{n}{cv\PYZus{}means}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cv\PYZus{}mean}\PY{p}{)}
              \PY{n}{tree\PYZus{}depths}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{depth}\PY{p}{)}
              \PY{n}{tree\PYZus{}models}\PY{p}{[}\PY{n}{depth}\PY{p}{]} \PY{o}{=} \PY{n}{dt\PYZus{}clf}
          
              \PY{c+c1}{\PYZsh{} update K with max CV Mean score}
              \PY{k}{if} \PY{n}{cv\PYZus{}mean} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}cv\PYZus{}mean}\PY{p}{:}
                  \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{=} \PY{n}{cv\PYZus{}mean}
                  \PY{n}{optimal\PYZus{}depth} \PY{o}{=} \PY{n}{depth}
              
          \PY{c+c1}{\PYZsh{} Plot the prediction accuracy andf 5 fold cross validation mean for each tree depth}
          \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{tree\PYZus{}depths}\PY{p}{,} \PY{n}{cv\PYZus{}means}\PY{p}{,}\PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}34495e}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{5 fold CV score Mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{tree\PYZus{}depths}\PY{p}{,} \PY{n}{accuracy}\PY{p}{,}\PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}16a085}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction Accuracy Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{n}{optimal\PYZus{}depth}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Optimal Depth \PYZhy{} }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{optimal\PYZus{}depth}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Set title, axis values and legends for the plot}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Plot of the prediction accuracy and CV mean score against max tree depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Max Tree Depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_76_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}121}]:} \PY{n}{DT\PYZus{}CLF} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Decision Tree, Depth\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{optimal\PYZus{}depth}\PY{p}{)} \PY{c+c1}{\PYZsh{} Model Title constant for Decision Tree Classifier}
          
          \PY{c+c1}{\PYZsh{} Report Prediction accuracy on Train and Validation Data sets}
          \PY{n}{dt\PYZus{}train\PYZus{}acc}\PY{p}{,} \PY{n}{dt\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{tree\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}depth}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{DT\PYZus{}CLF}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
          \PY{n}{dt\PYZus{}val\PYZus{}acc}\PY{p}{,} \PY{n}{dt\PYZus{}val\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{tree\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}depth}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{DT\PYZus{}CLF}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Decision Tree, Depth-3 Model - (Train) Prediction Accuracy: 0.82
Decision Tree, Depth-3 (Train) Cross Validation accuracy and 95 percent CI: 0.80 (+/- 0.06)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_77_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Decision Tree, Depth-3 Model - (Validation) Prediction Accuracy: 0.79

    \end{Verbatim}

    Gradient Boost Classifier

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}122}]:} \PY{c+c1}{\PYZsh{} Instantiate lists to hold estimator counts, accuracy and fitted model per estimator}
          \PY{n}{gb\PYZus{}estimators}\PY{o}{=}\PY{p}{[}\PY{p}{]}
          \PY{n}{accuracy} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{cv\PYZus{}means} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{gb\PYZus{}models} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
          
          \PY{n}{optimal\PYZus{}gb\PYZus{}estimator} \PY{o}{=} \PY{l+m+mi}{0}
          \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{=} \PY{l+m+mi}{0}
          
          \PY{c+c1}{\PYZsh{} Loop from estimator 1*32 to 40*32}
          \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{41}\PY{p}{)}\PY{p}{:}
              \PY{n}{est} \PY{o}{=} \PY{l+m+mi}{32}\PY{o}{*}\PY{n}{idx}
          
              \PY{c+c1}{\PYZsh{} Instantiate a Gradient Boosting classifier for the optimal depth using tol of 0.01}
              \PY{n}{gb\PYZus{}clf} \PY{o}{=} \PY{n}{GradientBoostingClassifier}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{o}{=} \PY{n}{est}\PY{p}{,} \PY{n}{validation\PYZus{}fraction}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.01}
                                                  \PY{p}{,} \PY{n}{n\PYZus{}iter\PYZus{}no\PYZus{}change}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{tol}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} Fit for the training data    }
              \PY{n}{gb\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
                  
              \PY{c+c1}{\PYZsh{} Check accuracy using prediction on the training data}
              \PY{n}{predictions} \PY{o}{=} \PY{n}{gb\PYZus{}clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{)}
              \PY{n}{acc\PYZus{}score} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
              \PY{n}{accuracy}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{acc\PYZus{}score}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} Check accuracy for 5\PYZhy{}fold cross validation test and take the mean}
              \PY{n}{cv\PYZus{}mean} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{gb\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
              \PY{n}{cv\PYZus{}means}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cv\PYZus{}mean}\PY{p}{)}
              \PY{n}{gb\PYZus{}estimators}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{est}\PY{p}{)}
              \PY{n}{gb\PYZus{}models}\PY{p}{[}\PY{n}{est}\PY{p}{]} \PY{o}{=} \PY{n}{gb\PYZus{}clf}
          
              \PY{c+c1}{\PYZsh{} update estimator with max CV Mean score if current score is atleast 0.001 greater than previous max}
              \PY{k}{if} \PY{n}{cv\PYZus{}mean} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{+}\PY{l+m+mf}{0.001}\PY{p}{:}
                  \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{=} \PY{n}{cv\PYZus{}mean}
                  \PY{n}{optimal\PYZus{}gb\PYZus{}estimator} \PY{o}{=} \PY{n}{est}
              
          \PY{c+c1}{\PYZsh{} Plot the prediction accuracy andf 5 fold cross validation mean for each tree depth}
          \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{gb\PYZus{}estimators}\PY{p}{,} \PY{n}{cv\PYZus{}means}\PY{p}{,}\PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}C30052}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{5 fold CV score Mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{gb\PYZus{}estimators}\PY{p}{,} \PY{n}{accuracy}\PY{p}{,}\PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}4C4A48}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction Accuracy Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{n}{optimal\PYZus{}gb\PYZus{}estimator}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Optimal Estimator \PYZhy{} }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{optimal\PYZus{}gb\PYZus{}estimator}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Set title, axis values and legends for the plot}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Plot of the prediction accuracy and CV mean score against optimal estimator n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{no of estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_79_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}123}]:} \PY{n}{GRAD\PYZus{}BOOST\PYZus{}CLF} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gradient Boost}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Model Title constant for Gradient Boost Classifier}
          
          \PY{c+c1}{\PYZsh{} Report Prediction accuracy on Train and Validation Data sets}
          \PY{n}{gb\PYZus{}train\PYZus{}acc}\PY{p}{,} \PY{n}{gb\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{gb\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}gb\PYZus{}estimator}\PY{p}{]}
                                                              \PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{GRAD\PYZus{}BOOST\PYZus{}CLF}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
          \PY{n}{gb\PYZus{}val\PYZus{}acc}\PY{p}{,} \PY{n}{gb\PYZus{}val\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{gb\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}gb\PYZus{}estimator}\PY{p}{]}
                                                          \PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{GRAD\PYZus{}BOOST\PYZus{}CLF}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Gradient Boost Model - (Train) Prediction Accuracy: 0.84
Gradient Boost (Train) Cross Validation accuracy and 95 percent CI: 0.81 (+/- 0.07)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_80_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Gradient Boost Model - (Validation) Prediction Accuracy: 0.80

    \end{Verbatim}

    Random Forest Classifier

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}124}]:} \PY{c+c1}{\PYZsh{} Instantiate lists to hold estimator counts, accuracy and fitted model per estimator}
          \PY{n}{rf\PYZus{}estimators}\PY{o}{=}\PY{p}{[}\PY{p}{]}
          \PY{n}{accuracy} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{cv\PYZus{}means} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{rf\PYZus{}models} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
          
          \PY{n}{optimal\PYZus{}rf\PYZus{}estimator} \PY{o}{=} \PY{l+m+mi}{0}
          \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{=} \PY{l+m+mi}{0}
          
          \PY{c+c1}{\PYZsh{} Loop from estimator 1*32 to 40*32}
          \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{41}\PY{p}{)}\PY{p}{:}
              \PY{n}{est} \PY{o}{=} \PY{l+m+mi}{32}\PY{o}{*}\PY{n}{idx}
              \PY{c+c1}{\PYZsh{}est = 2**idx}
              \PY{c+c1}{\PYZsh{}Instantiate a Decision Tree classifier for the optimal depth using gini index as the criterion}
              \PY{n}{rf\PYZus{}clf} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{o}{=} \PY{n}{est}\PY{p}{,} \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{n}{optimal\PYZus{}depth}\PY{p}{,} \PY{n}{criterion}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gini}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} Fit for the training data    }
              \PY{n}{rf\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
                  
              \PY{c+c1}{\PYZsh{} Check accuracy using prediction on the training data}
              \PY{n}{predictions} \PY{o}{=} \PY{n}{rf\PYZus{}clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{)}
              \PY{n}{acc\PYZus{}score} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
              \PY{n}{accuracy}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{acc\PYZus{}score}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} Check accuracy for 5\PYZhy{}fold cross validation test and take the mean}
              \PY{n}{cv\PYZus{}mean} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{rf\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
              \PY{n}{cv\PYZus{}means}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cv\PYZus{}mean}\PY{p}{)}
              \PY{n}{rf\PYZus{}estimators}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{est}\PY{p}{)}
              \PY{n}{rf\PYZus{}models}\PY{p}{[}\PY{n}{est}\PY{p}{]} \PY{o}{=} \PY{n}{rf\PYZus{}clf}
          
              \PY{c+c1}{\PYZsh{} update estimator with max CV Mean score if current score is atleast 0.001 greater than previous max}
              \PY{k}{if} \PY{n}{cv\PYZus{}mean} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{+}\PY{l+m+mf}{0.001}\PY{p}{:}
                  \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{=} \PY{n}{cv\PYZus{}mean}
                  \PY{n}{optimal\PYZus{}rf\PYZus{}estimator} \PY{o}{=} \PY{n}{est}
              
          \PY{c+c1}{\PYZsh{} Plot the prediction accuracy andf 5 fold cross validation mean for each tree depth}
          \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{rf\PYZus{}estimators}\PY{p}{,} \PY{n}{cv\PYZus{}means}\PY{p}{,}\PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}e74c3c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{5 fold CV score Mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{rf\PYZus{}estimators}\PY{p}{,} \PY{n}{accuracy}\PY{p}{,}\PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}2c3e50}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction Accuracy Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{n}{optimal\PYZus{}rf\PYZus{}estimator}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Optimal Estimator \PYZhy{} }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{optimal\PYZus{}rf\PYZus{}estimator}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Set title, axis values and legends for the plot}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Plot of the prediction accuracy and CV mean score against optimal estimator n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{no of estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_82_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}125}]:} \PY{n}{RAND\PYZus{}FOREST\PYZus{}CLF} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Random Forest}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Model Title constant for Random Forest Classifier}
          
          \PY{c+c1}{\PYZsh{} Report Prediction accuracy on Train and Validation Data sets}
          \PY{n}{rf\PYZus{}train\PYZus{}acc}\PY{p}{,} \PY{n}{rf\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{rf\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}rf\PYZus{}estimator}\PY{p}{]}
                                                              \PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{RAND\PYZus{}FOREST\PYZus{}CLF}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
          \PY{n}{rf\PYZus{}val\PYZus{}acc}\PY{p}{,} \PY{n}{rf\PYZus{}val\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{rf\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}rf\PYZus{}estimator}\PY{p}{]}
                                                          \PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{RAND\PYZus{}FOREST\PYZus{}CLF}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Random Forest Model - (Train) Prediction Accuracy: 0.83
Random Forest (Train) Cross Validation accuracy and 95 percent CI: 0.81 (+/- 0.07)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_83_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Random Forest Model - (Validation) Prediction Accuracy: 0.81

    \end{Verbatim}

    Boosting Adaboost Classifier

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}126}]:} \PY{c+c1}{\PYZsh{} Instantiate lists to hold estimator counts, accuracy and fitted model per estimator}
          \PY{n}{boost\PYZus{}estimators}\PY{o}{=}\PY{p}{[}\PY{p}{]}
          \PY{n}{accuracy} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{cv\PYZus{}means} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{boost\PYZus{}models} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
          
          \PY{n}{optimal\PYZus{}boost\PYZus{}estimator} \PY{o}{=} \PY{l+m+mi}{0}
          \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{=} \PY{l+m+mi}{0}
          \PY{c+c1}{\PYZsh{} Loop from estimator 1*32 to 40*32}
          \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{41}\PY{p}{)}\PY{p}{:}
              \PY{n}{est} \PY{o}{=} \PY{l+m+mi}{32}\PY{o}{*}\PY{n}{idx}
              
              \PY{c+c1}{\PYZsh{} Initialize a base learner decision tree with provided optimal depth}
              \PY{n}{base\PYZus{}clf} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{n}{optimal\PYZus{}depth}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} Initialize a ADA boost classifier using the base DT and provided hyper params}
              \PY{n}{ada\PYZus{}boost\PYZus{}clf} \PY{o}{=} \PY{n}{AdaBoostClassifier}\PY{p}{(}\PY{n}{base\PYZus{}estimator}\PY{o}{=}\PY{n}{base\PYZus{}clf}\PY{p}{,}\PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{n}{est}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} Train the classifier using the train data set}
              \PY{n}{ada\PYZus{}boost\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
                  
              \PY{c+c1}{\PYZsh{} Check accuracy using prediction on the training data}
              \PY{n}{predictions} \PY{o}{=} \PY{n}{ada\PYZus{}boost\PYZus{}clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{)}
              \PY{n}{acc\PYZus{}score} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
              \PY{n}{accuracy}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{acc\PYZus{}score}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} Check accuracy for 5\PYZhy{}fold cross validation test and take the mean}
              \PY{n}{cv\PYZus{}mean} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{ada\PYZus{}boost\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
              \PY{n}{cv\PYZus{}means}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cv\PYZus{}mean}\PY{p}{)}
              \PY{n}{boost\PYZus{}estimators}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{est}\PY{p}{)}
              \PY{n}{boost\PYZus{}models}\PY{p}{[}\PY{n}{est}\PY{p}{]} \PY{o}{=} \PY{n}{ada\PYZus{}boost\PYZus{}clf}
              
              \PY{c+c1}{\PYZsh{} update estimator with max CV Mean score if current score is atleast 0.001 greater than previous max}
              \PY{k}{if} \PY{n}{cv\PYZus{}mean} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{+}\PY{l+m+mf}{0.001}\PY{p}{:}
                  \PY{n}{max\PYZus{}cv\PYZus{}mean} \PY{o}{=} \PY{n}{cv\PYZus{}mean}
                  \PY{n}{optimal\PYZus{}boost\PYZus{}estimator} \PY{o}{=} \PY{n}{est}
                  
          \PY{c+c1}{\PYZsh{} Plot the prediction accuracy andf 5 fold cross validation mean for each tree depth}
          \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{boost\PYZus{}estimators}\PY{p}{,} \PY{n}{cv\PYZus{}means}\PY{p}{,}\PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}c0392b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{5 fold CV score Mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{boost\PYZus{}estimators}\PY{p}{,} \PY{n}{accuracy}\PY{p}{,}\PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}8e44ad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction Accuracy Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{n}{optimal\PYZus{}boost\PYZus{}estimator}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Optimal Estimator \PYZhy{} }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{optimal\PYZus{}boost\PYZus{}estimator}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Set title, axis values and legends for the plot}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Plot of the prediction accuracy and CV mean score against different estimator n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{no of estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_85_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}143}]:} \PY{n}{BOOSTING\PYZus{}CLF} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ada Boost}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Model Title constant for Boosting Classifier}
          
          \PY{c+c1}{\PYZsh{} Report Prediction accuracy on Train and Validation Data sets}
          \PY{n}{ada\PYZus{}boost\PYZus{}train\PYZus{}acc}\PY{p}{,} \PY{n}{ada\PYZus{}boost\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{boost\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}boost\PYZus{}estimator}\PY{p}{]}
                                                                            \PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{BOOSTING\PYZus{}CLF}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
          \PY{n}{ada\PYZus{}boost\PYZus{}val\PYZus{}acc}\PY{p}{,} \PY{n}{ada\PYZus{}boost\PYZus{}val\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{boost\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}boost\PYZus{}estimator}\PY{p}{]}
                                                                        \PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{BOOSTING\PYZus{}CLF}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Ada Boost Model - (Train) Prediction Accuracy: 0.83
Ada Boost (Train) Cross Validation accuracy and 95 percent CI: 0.81 (+/- 0.07)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_86_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Ada Boost Model - (Validation) Prediction Accuracy: 0.80

    \end{Verbatim}

    Support Vector Machine (SVM) Classifier using SVC

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}144}]:} \PY{c+c1}{\PYZsh{} Parameter grid to use during the Randomized Grid search CV}
          \PY{n}{parameters} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kernel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sigmoid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{expon}\PY{p}{(}\PY{n}{scale}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gamma}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{expon}\PY{p}{(}\PY{n}{scale}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{\PYZcb{}}
          
          \PY{c+c1}{\PYZsh{} svc base classifier to use}
          \PY{n}{svc} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{gamma}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{scale}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Instantiate a Randomized search CV instance and fit the training data to determine the best estimator}
          \PY{n}{rand\PYZus{}cv\PYZus{}clf} \PY{o}{=} \PY{n}{RandomizedSearchCV}\PY{p}{(}\PY{n}{svc}\PY{p}{,} \PY{n}{parameters}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
          \PY{n}{rand\PYZus{}cv\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{svc\PYZus{}clf} \PY{o}{=} \PY{n}{rand\PYZus{}cv\PYZus{}clf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}185}]:} \PY{n}{SVM\PYZus{}CLF} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Support Vector Machine}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Model Title constant for SVM Classifier}
          
          \PY{c+c1}{\PYZsh{} Train the best svg classifier obtained from grid search with the train data set}
          \PY{n}{svc\PYZus{}clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Report Prediction accuracy on Train and Validation Data sets}
          \PY{n}{svc\PYZus{}train\PYZus{}acc}\PY{p}{,} \PY{n}{svc\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{svc\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{SVM\PYZus{}CLF}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
          \PY{n}{svc\PYZus{}val\PYZus{}acc}\PY{p}{,} \PY{n}{svc\PYZus{}val\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{svc\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{SVM\PYZus{}CLF}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Support Vector Machine Model - (Train) Prediction Accuracy: 0.83
Support Vector Machine (Train) Cross Validation accuracy and 95 percent CI: 0.81 (+/- 0.06)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_89_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Support Vector Machine Model - (Validation) Prediction Accuracy: 0.82

    \end{Verbatim}

    Neural Network Classifier

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}146}]:} \PY{c+c1}{\PYZsh{} Get the models training history}
          \PY{k}{def} \PY{n+nf}{plot\PYZus{}nn\PYZus{}training\PYZus{}history}\PY{p}{(}\PY{n}{model\PYZus{}history}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Method to plot the accuracy and loss statistics from the model\PYZsq{}s training history}
          \PY{l+s+sd}{      }
          \PY{l+s+sd}{       Args:}
          \PY{l+s+sd}{           model\PYZus{}history: History object obtained from the trained model}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
              \PY{c+c1}{\PYZsh{} Initiate a figure for the plot}
              \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{24}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} Create a subplot for the Accuracy stats}
              \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{model\PYZus{}history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{model\PYZus{}history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{n}{bottom}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{top}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{validation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} Create a subplot for the Loss stats}
              \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{model\PYZus{}history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{model\PYZus{}history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
              
              \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}147}]:} \PY{c+c1}{\PYZsh{} Prepocess DF to NP arrays}
          \PY{n}{X\PYZus{}train\PYZus{}nn} \PY{o}{=} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{o}{.}\PY{n}{values}
          \PY{n}{y\PYZus{}train\PYZus{}nn} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{values}
          
          \PY{n}{X\PYZus{}val\PYZus{}nn} \PY{o}{=} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{o}{.}\PY{n}{values}
          \PY{n}{y\PYZus{}val\PYZus{}nn} \PY{o}{=} \PY{n}{y\PYZus{}val}\PY{o}{.}\PY{n}{values}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}148}]:} \PY{c+c1}{\PYZsh{} Instantiate a Sequential Model}
          \PY{n}{ann\PYZus{}model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Add a dropout regularization for input of the first hidden layer }
          \PY{n}{ann\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.25}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Add the 1st hidden layer}
          \PY{c+c1}{\PYZsh{} 100 Nodes, and relu activation}
          \PY{n}{ann\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}nn}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}
                              \PY{p}{,} \PY{n}{kernel\PYZus{}regularizer}\PY{o}{=}\PY{n}{keras}\PY{o}{.}\PY{n}{regularizers}\PY{o}{.}\PY{n}{l2}\PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Add the 2nd hidden layer}
          \PY{c+c1}{\PYZsh{} 50 Nodes, and relu activation}
          \PY{n}{ann\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{kernel\PYZus{}regularizer}\PY{o}{=}\PY{n}{keras}\PY{o}{.}\PY{n}{regularizers}\PY{o}{.}\PY{n}{l2}\PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{)}\PY{p}{)} 
          
          \PY{c+c1}{\PYZsh{} Add the 3rd hidden layer}
          \PY{c+c1}{\PYZsh{} 50 Nodes, and relu activation}
          \PY{n}{ann\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{kernel\PYZus{}regularizer}\PY{o}{=}\PY{n}{keras}\PY{o}{.}\PY{n}{regularizers}\PY{o}{.}\PY{n}{l2}\PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{)}\PY{p}{)} 
          
          \PY{c+c1}{\PYZsh{} Add the 4th hidden layer}
          \PY{c+c1}{\PYZsh{} 50 Nodes, and relu activation}
          \PY{n}{ann\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{kernel\PYZus{}regularizer}\PY{o}{=}\PY{n}{keras}\PY{o}{.}\PY{n}{regularizers}\PY{o}{.}\PY{n}{l2}\PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{)}\PY{p}{)} 
          
          \PY{c+c1}{\PYZsh{} Add the Output layer}
          \PY{c+c1}{\PYZsh{} 1 Node for 2 output classes, and sigmoid activation}
          \PY{n}{ann\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sigmoid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} 
          
          \PY{c+c1}{\PYZsh{} Compile the model with a SGD optimizer and binary\PYZus{}crossentropy loss function}
          \PY{n}{ann\PYZus{}model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sgd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{binary\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Fit the model with the training data with 2000 training epochs with a batch size of 128}
          \PY{n}{mod} \PY{o}{=} \PY{n}{ann\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}nn}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}nn}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{2000}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{validation\PYZus{}split}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}149}]:} \PY{c+c1}{\PYZsh{} Plot the accuracy and loss stats for the regulated model}
          \PY{n}{plot\PYZus{}nn\PYZus{}training\PYZus{}history}\PY{p}{(}\PY{n}{ann\PYZus{}model}\PY{o}{.}\PY{n}{history}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_94_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}150}]:} \PY{c+c1}{\PYZsh{} Evaluate the Regularized model accuracy on the training data}
          \PY{n}{ann\PYZus{}model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}nn}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}train\PYZus{}nn}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
1711/1711 [==============================] - ETA:  - 0s 12us/step

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}150}]:} [0.5087215290440237, 0.7849210992603536]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}151}]:} \PY{c+c1}{\PYZsh{} Evaluate the Regularized model accuracy on the validation data}
          \PY{n}{ann\PYZus{}model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{X\PYZus{}val\PYZus{}nn}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}val\PYZus{}nn}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
428/428 [==============================] - ETA:  - 0s 19us/step

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}151}]:} [0.49052348482274566, 0.8130841093642689]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}152}]:} \PY{n}{NN\PYZus{}CLF} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Neural Net}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} Model Title constant for NN Classifier}
          
          \PY{c+c1}{\PYZsh{} Report Prediction accuracy on Train and Validation Data sets}
          \PY{n}{ann\PYZus{}train\PYZus{}acc}\PY{p}{,} \PY{n}{ann\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{ann\PYZus{}model}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}nn}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}nn}\PY{p}{,} \PY{n}{NN\PYZus{}CLF}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
          \PY{n}{ann\PYZus{}val\PYZus{}acc}\PY{p}{,} \PY{n}{ann\PYZus{}val\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{ann\PYZus{}model}\PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}nn}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}nn}\PY{p}{,} \PY{n}{NN\PYZus{}CLF}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Neural Net Model - (Train) Prediction Accuracy: 0.78
Neural Net Model - (Validation) Prediction Accuracy: 0.81

    \end{Verbatim}

    Natural Language Processing (NLP)- Classification based on Topic
Modeling

\textbf{Topic modeling based on Hashtags used in the tweet text messages
of the user}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}155}]:} \PY{c+c1}{\PYZsh{} Constants with STOP LIST and noise symbols to extract and clean from the topics}
          \PY{n}{STOPLIST} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{n}{stopwords}\PY{o}{.}\PY{n}{words}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{english}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o}{+} \PY{n+nb}{list}\PY{p}{(}\PY{n}{ENGLISH\PYZus{}STOP\PYZus{}WORDS}\PY{p}{)}\PY{p}{)}
          \PY{n}{SYMBOLS} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{string}\PY{o}{.}\PY{n}{punctuation}\PY{p}{)}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{o}{+} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{...}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}156}]:} \PY{c+c1}{\PYZsh{} Class to implement Text transformation using TransformerMixin from sklearn}
          \PY{k}{class} \PY{n+nc}{CleanTextTransformer}\PY{p}{(}\PY{n}{TransformerMixin}\PY{p}{)}\PY{p}{:}
             \PY{k}{def} \PY{n+nf}{transform}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{transform\PYZus{}params}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{p}{[}\PY{n}{cleanText}\PY{p}{(}\PY{n}{text}\PY{p}{)} \PY{k}{for} \PY{n}{text} \PY{o+ow}{in} \PY{n}{X}\PY{p}{]}
             \PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{fit\PYZus{}params}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{n+nb+bp}{self}
             \PY{k}{def} \PY{n+nf}{get\PYZus{}params}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{deep}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
          
          \PY{c+c1}{\PYZsh{} utility method to clean text content}
          \PY{k}{def} \PY{n+nf}{cleanText}\PY{p}{(}\PY{n}{text}\PY{p}{)}\PY{p}{:}
              \PY{n}{text} \PY{o}{=} \PY{n}{text}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
              \PY{n}{text} \PY{o}{=} \PY{n}{text}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}
              \PY{k}{return} \PY{n}{text}
          
          \PY{c+c1}{\PYZsh{} utility method to tokenize text content}
          \PY{k}{def} \PY{n+nf}{tokenizeText}\PY{p}{(}\PY{n}{sample}\PY{p}{)}\PY{p}{:}
              \PY{n}{tokens} \PY{o}{=} \PY{n}{parser}\PY{p}{(}\PY{n}{sample}\PY{p}{)}
              \PY{n}{lemmas} \PY{o}{=} \PY{p}{[}\PY{p}{]}
              \PY{k}{for} \PY{n}{tok} \PY{o+ow}{in} \PY{n}{tokens}\PY{p}{:}
                  \PY{n}{lemmas}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{tok}\PY{o}{.}\PY{n}{lemma\PYZus{}}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)} \PY{k}{if} \PY{n}{tok}\PY{o}{.}\PY{n}{lemma\PYZus{}} \PY{o}{!=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}PRON\PYZhy{}}\PY{l+s+s2}{\PYZdq{}} \PY{k}{else} \PY{n}{tok}\PY{o}{.}\PY{n}{lower\PYZus{}}\PY{p}{)}
              \PY{n}{tokens} \PY{o}{=} \PY{n}{lemmas}
              \PY{n}{tokens} \PY{o}{=} \PY{p}{[}\PY{n}{tok} \PY{k}{for} \PY{n}{tok} \PY{o+ow}{in} \PY{n}{tokens} \PY{k}{if} \PY{n}{tok} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{STOPLIST}\PY{p}{]}
              \PY{n}{tokens} \PY{o}{=} \PY{p}{[}\PY{n}{tok} \PY{k}{for} \PY{n}{tok} \PY{o+ow}{in} \PY{n}{tokens} \PY{k}{if} \PY{n}{tok} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{SYMBOLS}\PY{p}{]}
              \PY{k}{return} \PY{n}{tokens}
          
          \PY{c+c1}{\PYZsh{} Utility method to report the top N Most informative topics from the topic based modeling classifier}
          \PY{k}{def} \PY{n+nf}{printNMostInformative}\PY{p}{(}\PY{n}{vectorizer}\PY{p}{,} \PY{n}{clf}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{:}
              \PY{n}{feature\PYZus{}names} \PY{o}{=} \PY{n}{vectorizer}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}
              \PY{n}{coefs\PYZus{}with\PYZus{}fns} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{feature\PYZus{}names}\PY{p}{)}\PY{p}{)}
              \PY{n}{topClass1} \PY{o}{=} \PY{n}{coefs\PYZus{}with\PYZus{}fns}\PY{p}{[}\PY{p}{:}\PY{n}{N}\PY{p}{]}
              \PY{n}{topClass2} \PY{o}{=} \PY{n}{coefs\PYZus{}with\PYZus{}fns}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{N} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
              \PY{n}{class1\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
              \PY{k}{for} \PY{n}{feat} \PY{o+ow}{in} \PY{n}{topClass1}\PY{p}{:}
                  \PY{n}{feat\PYZus{}dict} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
                  \PY{n}{feat\PYZus{}dict}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coefficient}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{feat}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                  \PY{n}{feat\PYZus{}dict}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Topic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{feat}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
                  \PY{n}{class1\PYZus{}lst}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{feat\PYZus{}dict}\PY{p}{)}
              \PY{n}{cls1\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{class1\PYZus{}lst}\PY{p}{)}
                  
              \PY{n}{class2\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
              \PY{k}{for} \PY{n}{feat} \PY{o+ow}{in} \PY{n}{topClass2}\PY{p}{:}
                  \PY{n}{feat\PYZus{}dict} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
                  \PY{n}{feat\PYZus{}dict}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coefficient}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{feat}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                  \PY{n}{feat\PYZus{}dict}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Topic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{feat}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
                  \PY{n}{class2\PYZus{}lst}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{feat\PYZus{}dict}\PY{p}{)}
              \PY{n}{cls2\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{class2\PYZus{}lst}\PY{p}{)}
              \PY{k}{return} \PY{n}{cls1\PYZus{}df}\PY{p}{,} \PY{n}{cls2\PYZus{}df}
\end{Verbatim}


    \textbf{Split Training and Validation data set from the consolidated
data set}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}157}]:} \PY{c+c1}{\PYZsh{} Extract the hash\PYZus{}tags feature containting the topic information and the response variable to form topic modeling data}
          \PY{n}{topic\PYZus{}data\PYZus{}df} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} Spli the data set into Traing and Validation sets}
          \PY{n}{topic\PYZus{}data\PYZus{}train}\PY{p}{,} \PY{n}{topic\PYZus{}data\PYZus{}val} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{topic\PYZus{}data\PYZus{}df}\PY{p}{,} \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.2}
                                                              \PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{90}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{topic\PYZus{}data\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Form the Predictor and Response data sets for train and validation}
          \PY{n}{X\PYZus{}topic\PYZus{}data\PYZus{}train} \PY{o}{=} \PY{n}{topic\PYZus{}data\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          \PY{n}{y\PYZus{}topic\PYZus{}data\PYZus{}train} \PY{o}{=} \PY{n}{topic\PYZus{}data\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          \PY{n}{X\PYZus{}topic\PYZus{}data\PYZus{}val} \PY{o}{=} \PY{n}{topic\PYZus{}data\PYZus{}val}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          \PY{n}{y\PYZus{}topic\PYZus{}data\PYZus{}val} \PY{o}{=} \PY{n}{topic\PYZus{}data\PYZus{}val}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \textbf{Fit the classification model to do Topic based classification}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}138}]:} \PY{c+c1}{\PYZsh{} Instantiate a vectorizer to break down the multiple topics from the messages thru tokenization}
          \PY{n}{vectorizer} \PY{o}{=} \PY{n}{CountVectorizer}\PY{p}{(}\PY{n}{tokenizer}\PY{o}{=}\PY{n}{tokenizeText}\PY{p}{,} \PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Instantiate a LinearSVC classifier to use as part of the topic based classification}
          \PY{n}{linear\PYZus{}svc\PYZus{}clf} \PY{o}{=} \PY{n}{LinearSVC}\PY{p}{(}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Define a pipeline that will clean the text content, vectorize it and fit the content using the SVC classifier}
          \PY{n}{tpipe} \PY{o}{=} \PY{n}{Pipeline}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cleanText}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{CleanTextTransformer}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vectorizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{vectorizer}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{clf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linear\PYZus{}svc\PYZus{}clf}\PY{p}{)}\PY{p}{]}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Train the model based on the aggregated topics}
          \PY{n}{tpipe}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}topic\PYZus{}data\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}topic\PYZus{}data\PYZus{}train}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Determine the top 10 topics used by the classifier during classification}
          \PY{n}{cls1\PYZus{}df}\PY{p}{,} \PY{n}{cls2\PYZus{}df} \PY{o}{=} \PY{n}{printNMostInformative}\PY{p}{(}\PY{n}{vectorizer}\PY{p}{,} \PY{n}{linear\PYZus{}svc\PYZus{}clf}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}142}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Top 10 topics used as features to predict: Class 1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{cls1\PYZus{}df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Top 10 topics used as features to predict: Class 1

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}142}]:}    Coefficient               Topic
          0    -0.910986            itrotter
          1    -0.835408            lolsided
          2    -0.835408  danielpadillaonggv
          3    -0.831407      socialsecurity
          4    -0.752205                 dad
          5    -0.560587                kita
          6    -0.558833               akhir
          7    -0.532553            ifwedate
          8    -0.502815        worldafroday
          9    -0.498887      tuesdaythought
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}140}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Top 10 topics used as features to predict: Class 2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{cls2\PYZus{}df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Top 10 topics used as features to predict: Class 2

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}140}]:}    Coefficient        Topic
          0     0.889049       trecru
          1     0.806954         2017
          2     0.543762   floodaware
          3     0.498687        kueez
          4     0.497928  freenazanin
          5     0.497926    realhuman
          6     0.497926  cuntscorner
          7     0.497926     snoopdog
          8     0.497926    eurekamag
          9     0.495390         jb17
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}158}]:} \PY{c+c1}{\PYZsh{} Make predictions using the topic modeling classifier on the Test data set and calculate accuracy scores}
          \PY{n}{tpreds\PYZus{}train} \PY{o}{=} \PY{n}{tpipe}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}topic\PYZus{}data\PYZus{}train}\PY{p}{)}
          \PY{n}{topic\PYZus{}train\PYZus{}acc} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}topic\PYZus{}data\PYZus{}train}\PY{p}{,} \PY{n}{tpreds\PYZus{}train}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Report the accuracy scores on Training data set}
          \PY{n}{printmd\PYZus{}color}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Topic Modeling Accuracy on }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{ data set \PYZhy{} }\PY{l+s+si}{\PYZpc{}0.2f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{TRAIN}\PY{p}{,} \PY{n}{topic\PYZus{}train\PYZus{}acc}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}2ecc71}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    Topic Modeling Accuracy on Train data set - 0.99

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}186}]:} \PY{n}{TOPIC\PYZus{}CLF} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Topic Model}\PY{l+s+s1}{\PYZsq{}}
          \PY{n}{topic\PYZus{}train\PYZus{}acc}\PY{p}{,} \PY{n}{topic\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{tpipe}\PY{p}{,} \PY{n}{X\PYZus{}topic\PYZus{}data\PYZus{}train}
                                                                    \PY{p}{,} \PY{n}{y\PYZus{}topic\PYZus{}data\PYZus{}train}\PY{p}{,} \PY{n}{TOPIC\PYZus{}CLF}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Topic Model Model - (Train) Prediction Accuracy: 0.99
Topic Model (Train) Cross Validation accuracy and 95 percent CI: 0.70 (+/- 0.07)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_108_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}159}]:} \PY{c+c1}{\PYZsh{} Make predictions using the topic modeling classifier on the validation data set and calculate accuracy scores}
          \PY{n}{tpreds\PYZus{}val} \PY{o}{=} \PY{n}{tpipe}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}topic\PYZus{}data\PYZus{}val}\PY{p}{)}
          \PY{n}{topic\PYZus{}val\PYZus{}acc} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}topic\PYZus{}data\PYZus{}val}\PY{p}{,} \PY{n}{tpreds\PYZus{}val}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Report the accuracy scores on validation data set}
          \PY{n}{printmd\PYZus{}color}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Topic Modeling Accuracy on }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{ data set \PYZhy{} }\PY{l+s+si}{\PYZpc{}0.2f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{VALIDATION}\PY{p}{,} \PY{n}{topic\PYZus{}val\PYZus{}acc}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}2ecc71}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    Topic Modeling Accuracy on Validation data set - 0.71

    
    \section{ STACKING - Ensembling of multi Classifier output using a meta
learner}\label{stacking---ensembling-of-multi-classifier-output-using-a-meta-learner}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}160}]:} \PY{c+c1}{\PYZsh{} Calculate the Stacking metalearner predictions using provided classifiers for the given data set}
          \PY{k}{def} \PY{n+nf}{calculate\PYZus{}metalearner\PYZus{}predictions}\PY{p}{(}\PY{n}{models\PYZus{}to\PYZus{}compare}\PY{p}{,} \PY{n}{X\PYZus{}data}\PY{p}{,} \PY{n}{y\PYZus{}data}\PY{p}{,} \PY{n}{data\PYZus{}type}\PY{o}{=}\PY{n}{TRAIN}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Method to calculate metalearner predictions using stacked output of classifiers}
          \PY{l+s+sd}{        Also calculate the prediction accuracy based on the true output}
          \PY{l+s+sd}{        }
          \PY{l+s+sd}{       Args:}
          \PY{l+s+sd}{           models\PYZus{}to\PYZus{}compare: Dictionary containing various fitted classifier models to stack}
          \PY{l+s+sd}{           X\PYZus{}data: Input predictor data set for the classifier models}
          \PY{l+s+sd}{           y\PYZus{}data: True output data for the provided predictors}
          \PY{l+s+sd}{           data\PYZus{}type: Type of the data set that is passed. Default is TRAIN}
          \PY{l+s+sd}{      }
          \PY{l+s+sd}{       Returns:}
          \PY{l+s+sd}{           stacked\PYZus{}preds: array of predictions from the metalearner for the given data set}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}    
              \PY{n}{individual\PYZus{}preds} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} DF to store predictions from all the classifier models}
              
              \PY{c+c1}{\PYZsh{} Iterate through all classifier models and predict to combine meta learner output}
              \PY{k}{for} \PY{n}{title}\PY{p}{,} \PY{n}{model} \PY{o+ow}{in} \PY{n}{models\PYZus{}to\PYZus{}compare}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                  \PY{c+c1}{\PYZsh{} Exclude PCA model for stacking}
                  \PY{k}{if} \PY{n}{title} \PY{o}{!=} \PY{n}{PCA\PYZus{}CLF} \PY{o+ow}{and} \PY{n}{title} \PY{o}{!=} \PY{n}{QDA\PYZus{}CLF}\PY{p}{:}
                      \PY{c+c1}{\PYZsh{} Concat predictions of the current model as a new column to the prediction set}
                      \PY{n}{individual\PYZus{}preds} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{individual\PYZus{}preds}\PY{p}{,} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}data}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{)}
                                                                                   \PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{n}{title}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
                  
              \PY{c+c1}{\PYZsh{} Initialize a stacked predictions of all 0 output}
              \PY{n}{stacked\PYZus{}preds} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{individual\PYZus{}preds}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} Iterate through each classifier prediction and stack all true predictions}
              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{individual\PYZus{}preds}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                  \PY{k}{if} \PY{n}{individual\PYZus{}preds}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.5}\PY{p}{:}
                      \PY{n}{stacked\PYZus{}preds}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}  
              
              \PY{c+c1}{\PYZsh{} Calculate accuracy based on stacked predictions}
              \PY{n}{acc\PYZus{}scr} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}data}\PY{p}{,} \PY{n}{stacked\PYZus{}preds}\PY{p}{)}
                 
              \PY{c+c1}{\PYZsh{} Report Accuracy}
              \PY{n}{h3\PYZus{}start} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}h3 style=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{padding\PYZhy{}top:10px;padding\PYZhy{}bottom:10px;padding\PYZhy{}left:5px;background\PYZhy{}color:\PYZsh{}1abc9c;}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}
              \PY{n}{h3\PYZus{}end} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}/h3\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}
              \PY{n}{printmd}\PY{p}{(}\PY{n}{h3\PYZus{}start} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Stacking Accuracy score on the }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ data set \PYZhy{} }\PY{l+s+si}{\PYZpc{}0.2f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{data\PYZus{}type}\PY{p}{,} \PY{n}{acc\PYZus{}scr}\PY{p}{)} \PY{o}{+} \PY{n}{h3\PYZus{}end}\PY{p}{)}
              
              \PY{k}{return} \PY{n}{stacked\PYZus{}preds}
          
          \PY{c+c1}{\PYZsh{} Utility method for formatted print}
          \PY{k}{def} \PY{n+nf}{printmd\PYZus{}color}\PY{p}{(}\PY{n}{text}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}1abc9c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
              \PY{n}{h3\PYZus{}start} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}h3 style=}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{padding\PYZhy{}top:10px;padding\PYZhy{}bottom:10px;padding\PYZhy{}left:5px;background\PYZhy{}color:}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n}{color} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{;}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}
              \PY{n}{h3\PYZus{}end} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}/h3\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}
              \PY{n}{printmd}\PY{p}{(}\PY{n}{h3\PYZus{}start} \PY{o}{+} \PY{n}{text} \PY{o}{+} \PY{n}{h3\PYZus{}end}\PY{p}{)}
\end{Verbatim}


    \section{ Train and Validation - Prediction Results and
Stats}\label{train-and-validation---prediction-results-and-stats}

    Comparison of prediction accuracies across classifiers - TRAIN dataset

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}161}]:} \PY{c+c1}{\PYZsh{} Stacking Classification Accuracy score for Training Data}
          \PY{n}{stacked\PYZus{}predictions\PYZus{}train} \PY{o}{=} \PY{n}{calculate\PYZus{}metalearner\PYZus{}predictions}\PY{p}{(}\PY{n}{models\PYZus{}to\PYZus{}compare}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
\end{Verbatim}


    Stacking Accuracy score on the Train data set - 0.83

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}162}]:} \PY{c+c1}{\PYZsh{} Compare classifier accuracy stats for Training data}
          \PY{n}{renderAccuracyStatsIable}\PY{p}{(}\PY{n}{acc\PYZus{}stats\PYZus{}table\PYZus{}data}\PY{p}{,} \PY{n}{TRAIN}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
** Train ** Comparison Table of Accuracy stats for different classifiation models

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_115_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Comparison of prediction accuracies across classifiers - VALIDATION
Dataset

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}163}]:} \PY{c+c1}{\PYZsh{} Stacking Classification Accuracy score for Validation Data}
          \PY{n}{stacked\PYZus{}predictions\PYZus{}val} \PY{o}{=} \PY{n}{calculate\PYZus{}metalearner\PYZus{}predictions}\PY{p}{(}\PY{n}{models\PYZus{}to\PYZus{}compare}\PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    Stacking Accuracy score on the Validation data set - 0.82

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}164}]:} \PY{c+c1}{\PYZsh{} Compare classifier accuracy stats for Validation data}
          \PY{n}{renderAccuracyStatsIable}\PY{p}{(}\PY{n}{acc\PYZus{}stats\PYZus{}table\PYZus{}data}\PY{p}{,} \PY{n}{VALIDATION}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
** Validation ** Comparison Table of Accuracy stats for different classifiation models

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_118_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{ Test Data Preparation and Models
validation}\label{test-data-preparation-and-models-validation}

Data Preparation

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}165}]:} \PY{c+c1}{\PYZsh{} Read the Pre\PYZhy{}classified data set containing twitter user labeled as either Bots (1) or Human Users (0)}
          \PY{n}{labelled\PYZus{}df\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cresci\PYZus{}2017\PYZus{}test\PYZus{}prelabelled.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Determine no of users in the labelled data classified as Bots(1) and who are determined as Human Users (0)}
          \PY{n}{labelled\PYZus{}df\PYZus{}test}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}165}]:}          user\_id
          bot\_flg         
          0            526
          1            392
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}166}]:} \PY{c+c1}{\PYZsh{} MARKED FOR EXECUTION AS NEEDED \PYZhy{} NOTE \PYZhy{} EXTRACTION TAKES A LONG TIME}
          \PY{c+c1}{\PYZsh{} Download the latest 3000 tweets as json for each user in cresci\PYZus{}2017\PYZus{}test\PYZus{}prelabelled.csv (labelled\PYZus{}df\PYZus{}test)}
          
          \PY{c+c1}{\PYZsh{} success\PYZus{}users\PYZus{}test, restricted\PYZus{}users\PYZus{}test = extract\PYZus{}tweets\PYZus{}for\PYZus{}users(labelled\PYZus{}df\PYZus{}test, TWEET\PYZus{}EXTRACT\PYZus{}LOCATION\PYZus{}TEST)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}167}]:} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{TWEET\PYZus{}EXTRACT\PYZus{}LOCATION\PYZus{}TEST} \PY{o}{+} \PY{n}{CONSOLIDATED\PYZus{}FEATURES}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} MARKED FOR EXECUTION AS NEEDED \PYZhy{} NOTE \PYZhy{} PARSING and CONSOLIDATES TAKES A LONG TIME}
          \PY{c+c1}{\PYZsh{} Extract and consolidate all features for every user}
          \PY{c+c1}{\PYZsh{}consolidated\PYZus{}ftrs\PYZus{}df\PYZus{}test = consolidate\PYZus{}features\PYZus{}at\PYZus{}user\PYZus{}level(TWEET\PYZus{}EXTRACT\PYZus{}LOCATION\PYZus{}TEST)}
          
          \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df\PYZus{}test} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df\PYZus{}test}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n}{frac}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df\PYZus{}test}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}167}]:}             user\_id  default\_profile  default\_profile\_image  favourites\_count  followers\_count  friends\_count  listed\_count  statuses\_count  geo\_enabled  protected    verified  has\_extended\_profile  is\_translator  senti\_polarity  senti\_subjectivity  place\_reference  possibly\_sensitive  is\_a\_reply  is\_quote\_status  is\_a\_retweet  favorite\_count  retweet\_count  hash\_tags\_cnt   media\_cnt  url\_ref\_cnt  symbols\_cnt  user\_ref\_cnt  tweet\_freq\_hrs     bot\_flg
          count  7.590000e+02       759.000000             759.000000        759.000000       759.000000     759.000000    759.000000      759.000000   759.000000      759.0  759.000000            759.000000     759.000000      759.000000          759.000000       759.000000          759.000000  759.000000       759.000000    759.000000      759.000000     759.000000     759.000000  759.000000   759.000000   759.000000    759.000000      759.000000  759.000000
          mean   2.422529e+09         0.351779               0.005270       7017.304348       723.781291     428.158103     18.015810    18922.671937     0.349144        0.0    0.001318              0.309618       0.001318        0.088758            0.270220         0.021095            0.006884    0.150647         0.042057      0.192339        1.432572    3001.980435       0.312464    0.149235     0.415457     0.000245      0.429975       26.046881    0.474308
          std    6.407658e+08         0.477840               0.072452      17609.872465      3244.197278    1592.319104     31.884931    28996.646650     0.477014        0.0    0.036298              0.462641       0.036298        0.057007            0.096327         0.079940            0.079025    0.161396         0.076723      0.262976       12.590002    9142.138684       0.669913    0.204007     0.428245     0.003997      0.445413      128.101064    0.499669
          min    8.572704e+08         0.000000               0.000000          0.000000         2.000000       0.000000      0.000000        4.000000     0.000000        0.0    0.000000              0.000000       0.000000       -0.078000            0.000000         0.000000            0.000000    0.000000         0.000000      0.000000        0.000000       0.000000       0.000000    0.000000     0.000000     0.000000      0.000000        0.000000    0.000000
          25\%    2.241075e+09         0.000000               0.000000          1.000000        43.000000      61.000000      0.000000     1117.500000     0.000000        0.0    0.000000              0.000000       0.000000        0.052000            0.209000         0.000000            0.000000    0.000000         0.000000      0.000000        0.031500       0.015000       0.021000    0.000000     0.055000     0.000000      0.000000        2.077000    0.000000
          50\%    2.548562e+09         0.000000               0.000000         57.000000       157.000000     160.000000      5.000000     8329.000000     0.000000        0.0    0.000000              0.000000       0.000000        0.080000            0.285000         0.000000            0.000000    0.111000         0.000000      0.019000        0.094000       8.667000       0.070000    0.091000     0.198000     0.000000      0.333000        7.182000    0.000000
          75\%    2.886263e+09         1.000000               0.000000       5345.000000       472.500000     364.500000     23.000000    23218.000000     1.000000        0.0    0.000000              1.000000       0.000000        0.117000            0.337500         0.000000            0.000000    0.268500         0.052000      0.324000        0.451000    1729.406000       0.234000    0.203500     1.002000     0.000000      0.727500       19.076000    1.000000
          max    4.331280e+09         1.000000               1.000000     202812.000000     59649.000000   38571.000000    346.000000   259200.000000     1.000000        0.0    1.000000              1.000000       1.000000        0.422000            0.650000         0.831000            0.985000    0.924000         0.521000      1.000000      199.497000  170042.000000       4.493000    1.000000     1.338000     0.107000      4.016000     2807.125000    1.000000
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}168}]:} \PY{c+c1}{\PYZsh{} Drop user\PYZus{}id from the dataframe as that is not needed for classification}
          \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df\PYZus{}test} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df\PYZus{}test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{user\PYZus{}id}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{errors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Drop the categorical values that does not seem to be providing value based on EDA}
          \PY{n}{cols\PYZus{}to\PYZus{}drop} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{protected}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{verified}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}translator}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{possibly\PYZus{}sensitive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{place\PYZus{}reference}\PY{l+s+s1}{\PYZsq{}}
                         \PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{user\PYZus{}lang}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{default\PYZus{}profile\PYZus{}image}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{default\PYZus{}profile}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{symbols\PYZus{}cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          \PY{n}{processed\PYZus{}data\PYZus{}df\PYZus{}test} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df\PYZus{}test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{cols\PYZus{}to\PYZus{}drop}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{errors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Split the test data after scaling through normalization}
          \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{dummyX}\PY{p}{,} \PY{n}{dummyY} \PY{o}{=} \PY{n}{split\PYZus{}train\PYZus{}and\PYZus{}test}\PY{p}{(}\PY{n}{processed\PYZus{}data\PYZus{}df\PYZus{}test}\PY{p}{,} \PY{n}{split\PYZus{}ratio}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{)}
\end{Verbatim}


    Prediction Results on Test Data

    \textbf{Ensembing based on Stacking}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}169}]:} \PY{n}{stacked\PYZus{}predictions\PYZus{}test} \PY{o}{=} \PY{n}{calculate\PYZus{}metalearner\PYZus{}predictions}\PY{p}{(}\PY{n}{models\PYZus{}to\PYZus{}compare}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{TEST}\PY{p}{)}
\end{Verbatim}


    Stacking Accuracy score on the Test data set - 0.81

    
    Comparison of prediction accuracies across classifiers - TEST Dataset

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}183}]:} \PY{c+c1}{\PYZsh{} Compare classifier accuracy stats for Test data}
          \PY{n}{renderAccuracyStatsIable}\PY{p}{(}\PY{n}{acc\PYZus{}stats\PYZus{}table\PYZus{}data}\PY{p}{,} \PY{n}{TEST}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
** Test ** Comparison Table of Accuracy stats for different classifiation models

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_128_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Classification based on Topic Modeling - TEST Dataset

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}171}]:} \PY{c+c1}{\PYZsh{} Extract the Hast\PYZus{}tags feature containing the topics information and the response feature}
          \PY{n}{topic\PYZus{}data\PYZus{}df\PYZus{}test} \PY{o}{=} \PY{n}{consolidated\PYZus{}ftrs\PYZus{}df\PYZus{}test}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} Form the test predictor and response data set for topic modeling based classification}
          \PY{n}{X\PYZus{}topic\PYZus{}data\PYZus{}test} \PY{o}{=} \PY{n}{topic\PYZus{}data\PYZus{}df\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash\PYZus{}tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          \PY{n}{y\PYZus{}topic\PYZus{}data\PYZus{}test} \PY{o}{=} \PY{n}{topic\PYZus{}data\PYZus{}df\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}flg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} Predict based on the test topics}
          \PY{n}{topic\PYZus{}test\PYZus{}preds} \PY{o}{=} \PY{n}{tpipe}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}topic\PYZus{}data\PYZus{}test}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Report Accuracy score from the Test predictions}
          \PY{n}{topic\PYZus{}test\PYZus{}acc} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}topic\PYZus{}data\PYZus{}test}\PY{p}{,} \PY{n}{topic\PYZus{}test\PYZus{}preds}\PY{p}{)}
          \PY{n}{printmd\PYZus{}color}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Topic Modeling Accuracy on }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{ data set \PYZhy{} }\PY{l+s+si}{\PYZpc{}0.2f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{TEST}\PY{p}{,} \PY{n}{topic\PYZus{}test\PYZus{}acc}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}2ecc71}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    Topic Modeling Accuracy on Test data set - 0.85

    
    Prediction accuracies across base classifiers - TEST Dataset

    \textbf{Logistic Classifier CV}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}172}]:} \PY{n}{lcv\PYZus{}test\PYZus{}acc}\PY{p}{,} \PY{n}{lcv\PYZus{}test\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{lcv\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{LOGISTIC\PYZus{}CLF}\PY{p}{,} \PY{n}{TEST}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Logistic CV Model - (Test) Prediction Accuracy: 0.80

    \end{Verbatim}

    \textbf{Linear Discriminant Analysis (LDA)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}173}]:} \PY{n}{lda\PYZus{}test\PYZus{}acc}\PY{p}{,} \PY{n}{lda\PYZus{}test\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{lda}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{LDA\PYZus{}CLF}\PY{p}{,} \PY{n}{TEST}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
LDA Model - (Test) Prediction Accuracy: 0.80

    \end{Verbatim}

    \textbf{Quadratic Discriminant Analysis (QDA)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}174}]:} \PY{n}{qda\PYZus{}test\PYZus{}acc}\PY{p}{,} \PY{n}{qda\PYZus{}test\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{qda}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{QDA\PYZus{}CLF}\PY{p}{,} \PY{n}{TEST}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
QDA Model - (Test) Prediction Accuracy: 0.68

    \end{Verbatim}

    \textbf{k Nearest Neigbors Classifier (kNN)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}175}]:} \PY{n}{knn\PYZus{}test\PYZus{}acc}\PY{p}{,} \PY{n}{knn\PYZus{}test\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{knn\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}k}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{KNN\PYZus{}CLF}\PY{p}{,} \PY{n}{TEST}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
kNN, K-13 Model - (Test) Prediction Accuracy: 0.81

    \end{Verbatim}

    \textbf{Support Vector Machine (SVM) using SVC}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}176}]:} \PY{n}{svc\PYZus{}test\PYZus{}acc}\PY{p}{,} \PY{n}{svc\PYZus{}test\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{svc\PYZus{}clf}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{SVM\PYZus{}CLF}\PY{p}{,} \PY{n}{TEST}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Support Vector Machine Model - (Test) Prediction Accuracy: 0.79

    \end{Verbatim}

    \textbf{Decision Tree}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}177}]:} \PY{n}{dt\PYZus{}test\PYZus{}acc}\PY{p}{,} \PY{n}{dt\PYZus{}test\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{tree\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}depth}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{DT\PYZus{}CLF}\PY{p}{,} \PY{n}{TEST}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Decision Tree, Depth-3 Model - (Test) Prediction Accuracy: 0.82

    \end{Verbatim}

    \textbf{Gradient Boost Ensembling}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}178}]:} \PY{n}{gb\PYZus{}test\PYZus{}acc}\PY{p}{,} \PY{n}{gb\PYZus{}test\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{gb\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}gb\PYZus{}estimator}\PY{p}{]}
                                                            \PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{GRAD\PYZus{}BOOST\PYZus{}CLF}\PY{p}{,} \PY{n}{TEST}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Gradient Boost Model - (Test) Prediction Accuracy: 0.81

    \end{Verbatim}

    \textbf{Random Forest Ensembling}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}179}]:} \PY{n}{rf\PYZus{}test\PYZus{}acc}\PY{p}{,} \PY{n}{rf\PYZus{}test\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{rf\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}rf\PYZus{}estimator}\PY{p}{]}
                                                            \PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{RAND\PYZus{}FOREST\PYZus{}CLF}\PY{p}{,} \PY{n}{TEST}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Random Forest Model - (Test) Prediction Accuracy: 0.85

    \end{Verbatim}

    \textbf{Adaptive Boosting Ensembling}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}180}]:} \PY{n}{boost\PYZus{}test\PYZus{}acc}\PY{p}{,} \PY{n}{boost\PYZus{}test\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{boost\PYZus{}models}\PY{p}{[}\PY{n}{optimal\PYZus{}boost\PYZus{}estimator}\PY{p}{]}
                                                                  \PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{BOOSTING\PYZus{}CLF}\PY{p}{,} \PY{n}{TEST}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Ada Boost Model - (Test) Prediction Accuracy: 0.81

    \end{Verbatim}

    \textbf{Neural Networks}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}181}]:} \PY{n}{ann\PYZus{}test\PYZus{}acc}\PY{p}{,} \PY{n}{ann\PYZus{}test\PYZus{}cv} \PY{o}{=} \PY{n}{get\PYZus{}classifier\PYZus{}accuracy}\PY{p}{(}\PY{n}{ann\PYZus{}model}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{NN\PYZus{}CLF}\PY{p}{,} \PY{n}{TEST}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Neural Net Model - (Test) Prediction Accuracy: 0.82

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}182}]:} \PY{n}{stacked\PYZus{}predictions\PYZus{}test} \PY{o}{=} \PY{n}{calculate\PYZus{}metalearner\PYZus{}predictions}\PY{p}{(}\PY{n}{models\PYZus{}to\PYZus{}compare}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}scale}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{TEST}\PY{p}{)}
\end{Verbatim}


    Stacking Accuracy score on the Test data set - 0.81

    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
